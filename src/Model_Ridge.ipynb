{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline using Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sumy import evaluation\n",
    "from sumy.models import dom\n",
    "from sumy.nlp import tokenizers\n",
    "from stemming.porter2 import stem\n",
    "from os import listdir\n",
    "import os.path\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import copy\n",
    "import pickle\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import nltk.data\n",
    "\n",
    "java_path = \"C:/Program Files/Java/jdk1.7.0_71/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Feature Extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\".\\Others\\Features.png\" alt=\"HTML5 Icon\" width=\"800\" height=\"500\", style=\"display: ;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_root_dir = '..\\data\\DUC2001'\n",
    "annotation_file = 'annotations.txt'\n",
    "txt_opn_tag = '<TEXT>'\n",
    "txt_close_tag = '</TEXT>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_and_its_files(data_root_dir,annotation_file):\n",
    "    '''Get a Cluster and the file names associated with it\n",
    "       Returns a dictionary of the form { cluster_1 : [file1,file2,file3....], cluster_2 : [file1,file2,file3....] }'''    \n",
    "    \n",
    "    f = open(data_root_dir + '\\\\' + annotation_file,'r')\n",
    "    \n",
    "    clust_files = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    for line in f.readlines():\n",
    "        cur_line = line.split(';')[0]\n",
    "        clust_name = cur_line.split('@')[1]\n",
    "        file_name = cur_line.split('@')[0]\n",
    "        \n",
    "        clust_files[clust_name].append(file_name)\n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "    return clust_files\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AP900322-0200', 'FBIS-41815', 'FBIS-45908', 'FT921-9310', 'FT931-3883', 'FT933-8272', 'FT941-575', 'LA042290-0104', 'LA060490-0083', 'WSJ910107-0139']\n"
     ]
    }
   ],
   "source": [
    "clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "print clust_files['mad cow disease']\n",
    "clust_list = clust_files.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_from_doc(document_path,txt_opn_tag,txt_close_tag):\n",
    "    \n",
    "    f = open(document_path,'r')\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    start = content.index(txt_opn_tag) + len(txt_opn_tag)\n",
    "    end   = content.index(txt_close_tag)\n",
    "    \n",
    "    return content[start:end]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_txt(text,nltk_flag=True,ner_flag=False):\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    if ner_flag == True:        \n",
    "        tokenizedList = re.split('[^a-zA-Z]+', text.lower())\n",
    "        return tokenizedList\n",
    "    \n",
    "    if nltk_flag == False:\n",
    "        #return [x.lower() for x in re.findall(r\"\\w+\", text)]\n",
    "\n",
    "        tokenizedList = re.split('\\W+', text.lower())\n",
    "        return [unicode(x,'utf-8') for x in tokenizedList if x != '' and x != '\\n' and x != u'\\x85' and x != '\\r' and x != '_']\n",
    "    else:\n",
    "        return nltk.word_tokenize(unicode(text,'utf-8')) \n",
    "        #return [x for x in toks if x != '' and x != '\\n' and x != u'\\x85' and x != '\\r' and x != '_' and x!= ',' and x != '.']    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'this', 'is', 'this', 'cool', 'i', 'don', 't', 'know']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_txt('What is this ?? Is this _ cool ? I don\\'t know',nltk_flag=True,ner_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 1 : Term frequency over the cluster(TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_term_freqs(data_root_dir,annotation_file,stop_words=None) :\n",
    "    '''Get the term freqs of words in clusters. The term freqs are unique to clusters.\n",
    "    Returns a dict of form {clust1 : {word1 : 2, word2 :3...},clust2 : {word1 : 2, word2 :3..} ......}'''\n",
    "        \n",
    "    #Check about stop_words\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_term_freq = defaultdict(defaultdict)\n",
    "    \n",
    "    \n",
    "    for clust,files in clust_files.iteritems():\n",
    "        term_freq = defaultdict(int)\n",
    "        \n",
    "        for doc in files:\n",
    "            doc_path = data_root_dir + '\\\\' + doc\n",
    "            txt = get_text_from_doc(doc_path,txt_opn_tag,txt_close_tag)\n",
    "            doc_tokens = tokenize_txt(txt)\n",
    "            \n",
    "            for token in doc_tokens:\n",
    "                term_freq[token] += 1\n",
    "        \n",
    "        clust_term_freq[clust] = term_freq\n",
    "    \n",
    "    return clust_term_freq\n",
    "            \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {u'all': 1, u'Union': 1, u'Kretzschmar': 1, u'Switzerland': 1, u'per': 1, u'human': 1, u'still': 1, u'decisions': 1, u'its': 1, u'European': 1, u'Jakob': 1, u'one': 1, u'March': 1, u'(': 2, u'had': 2, u',': 10, u'should': 1, u'to': 6, u'safeguards': 1, u'do': 1, u'popularly': 1, u'affected': 1, u'diseases': 1, u'than': 1, u'government': 1, u'very': 1, u'100,000': 1, u'scientists': 1, u'possible': 1, u'Gottingen': 1, u'were': 3, u'know': 1, u'not': 3, u'affect': 2, u'existing': 1, u'countries': 1, u'medicines': 1, u'50': 1, u'whether': 1, u'transmitted': 2, u'minimal': 1, u'ban': 2, u'Contaminated': 1, u'because': 1, u'humans': 4, u'bovine': 1, u'connections': 1, u'likely': 1, u'catching': 1, u'are': 1, u'encephalopathy': 1, u'further': 1, u'institutes': 1, u'agriculture': 1, u'concern': 1, u'universities': 1, u'project': 1, u'said': 3, u'imported': 3, u'for': 2, u'1992': 1, u'recorded': 1, u'expressed': 1, u'research': 4, u'I': 1, u'health': 1, u'between': 1, u'new': 1, u'contaminated': 2, u'University': 1, u'announced': 1, u'available': 1, u'be': 7, u'we': 1, u'Professor': 1, u'pushing': 1, u'EU': 2, u'by': 2, u'official': 1, u'on': 2, u'about': 1, u'last': 1, u'would': 1, u'origins': 1, u'launch': 1, u'of': 11, u'30': 1, u'discussed': 1, u'figures': 1, u'argue': 1, u'or': 2, u'comes': 1, u\"'The\": 1, u'Gerstmann': 1, u'danger': 1, u'year': 1, u'ministers': 2, u'beings': 1, u'initiative': 1, u'been': 1, u'rarely': 1, u'from': 4, u'beef': 6, u'debilitates': 1, u'union': 1, u'there': 2, u'two': 2, u'cent': 1, u'.': 11, u'2': 1, u'way': 1, u'Hans': 1, u'BSE': 5, u'meeting': 1, u'more': 1, u'-may': 1, u'spongiform': 1, u'that': 8, u'brains': 1, u'sufficient': 1, u'but': 1, u'personally': 1, u'known': 2, u'cases': 2, u'with': 1, u'eat': 1, u'2,092': 1, u'Seven': 1, u'made': 2, u'animals': 1, u'cow': 2, u'transmissible': 1, u'German': 4, u'ministry': 2, u'as': 3, u'will': 2, u'Britain': 2, u'can': 3, u'country': 2, u'Straussler': 1, u'at': 1, u'and': 6, u'non-existent': 2, u'imports': 2, u'is': 4, u'cattle': 3, u'it': 2, u'evidence': 1, u'examine': 2, u'ingredients': 1, u'have': 1, u'in': 3, u'Several': 1, u'technology': 1, u'any': 1, u'result': 1, u\"'mad\": 1, u'syndrome': 1, u'no': 1, u')': 2, u'-': 3, u'other': 2, u'Germany': 2, u'take': 1, u'which': 3, u'A': 1, u\"'s\": 2, u'arguing': 1, u'who': 2, u'yesterday': 1, u'British': 4, u'sponsored': 1, u'The': 3, u'13': 1, u'died': 1, u'conclusive': 1, u'a': 7, u\"'\": 3, u\"'However\": 1, u'Creutzfeldt': 1, u'disease': 5, u'think': 1, u'veal': 1, u'In': 1, u'tonnes': 2, u'the': 11})\n"
     ]
    }
   ],
   "source": [
    "clust_word_tfs = get_term_freqs(data_root_dir,annotation_file)\n",
    "print clust_word_tfs['cattle disease']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Feature 2 : Total document number in the datasets, divided by the frequency of documents which contains this word (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_doc_freqs(data_root_dir,annotation_file):\n",
    "    \n",
    "    '''Return a dictionary of the form {word1 : df1 , word2 : df2 ...}'''\n",
    "    '''Example : {furazabol : 154.5 , the : 1.00032}'''\n",
    "    \n",
    "    data_root_dir += '\\\\'\n",
    "    \n",
    "    docs =  [file_name for _,__,file_name in os.walk(data_root_dir)][0]\n",
    "    \n",
    "    if annotation_file in docs:\n",
    "        docs.remove(annotation_file)        \n",
    "    \n",
    "    inverted_index  = defaultdict(set)\n",
    "    \n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_path = data_root_dir + doc        \n",
    "        txt = get_text_from_doc(doc_path,txt_opn_tag,txt_close_tag)\n",
    "        doc_tokens = tokenize_txt(txt)\n",
    "        \n",
    "        for token in doc_tokens:\n",
    "            inverted_index[token].add(doc)\n",
    "    \n",
    "    \n",
    "    \n",
    "    no_of_docs = len(docs)\n",
    "    idf_dict = defaultdict(float)\n",
    "    \n",
    "    for term,doc_lst in inverted_index.iteritems():\n",
    "        idf_dict[term] = float(no_of_docs) / len(doc_lst)\n",
    "    \n",
    "    return idf_dict\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154.5\n",
      "1.00324675325\n"
     ]
    }
   ],
   "source": [
    "doc_freqs = get_doc_freqs(data_root_dir,annotation_file)\n",
    "print doc_freqs['furazabol']\n",
    "print doc_freqs['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 3 : The frequency of documents which contains this word in the current cluster (CF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clusterwise_dfs(data_root_dir,annotation_file):\n",
    "    \n",
    "    '''Return a dictionary of the form : {clust1 : (word1 : df1,word2 :df2 .....) , clust1 : (word3 : df3,word2 :df3 .....)}'''\n",
    "    '''Note that the document frequencies of term are calculated clusterwise, and not on the whole dataset'''\n",
    "    \n",
    "    clust_doc_freqs = defaultdict(defaultdict)\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():\n",
    "        inverted_index  = defaultdict(set)\n",
    "        \n",
    "        for doc in files:\n",
    "            doc_path = data_root_dir + '\\\\' + doc\n",
    "            txt = get_text_from_doc(doc_path,txt_opn_tag,txt_close_tag)\n",
    "            doc_tokens = tokenize_txt(txt)\n",
    "            \n",
    "            for token in doc_tokens:\n",
    "                inverted_index[token].add(doc)\n",
    "        \n",
    "        \n",
    "        clust_df = defaultdict(int)\n",
    "        \n",
    "        for term,doc_lst in inverted_index.iteritems():\n",
    "            clust_df[term] =  len(doc_lst)\n",
    "        \n",
    "        clust_doc_freqs[clust] = clust_df\n",
    "    \n",
    "    return clust_doc_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u',', 10), (u'encephalopathy', 10), (u'cow', 10), (u'and', 10), (u'.', 10), (u'in', 10), (u'the', 10), (u'has', 10), (u'for', 10), (u\"'s\", 10), (u'that', 10), (u'were', 10), (u'spongiform', 10), (u'of', 10), (u'with', 10), (u'as', 10), (u'to', 10), (u'a', 10), (u'be', 10), (u'from', 10)]\n"
     ]
    }
   ],
   "source": [
    "clust_dfs = get_clusterwise_dfs(data_root_dir,annotation_file)\n",
    "print sorted(clust_dfs['mad cow disease'].items(),key=operator.itemgetter(1),reverse=True)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 4 : A 4-dimension binary vector indicates whether the word is a noun, a verb, an adjective or an adverb. If the word has\n",
    "another part-of-speech, the vector is all-zero  (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_short_tag(long_tag,valid_pos=['NN','VB','JJ','RB']):      \n",
    "    '''Truncate long_tag to get its first 2 chars. If a valid POS, return first 2 chars. else return OT (Other)'''\n",
    "    '''Valid POS are NN,VB,JJ,RB'''\n",
    "    \n",
    "    valid_pos_lst = valid_pos\n",
    "       \n",
    "    long_tag = str.upper(long_tag[0:2])\n",
    "    \n",
    "    if long_tag in valid_pos_lst:\n",
    "        return long_tag\n",
    "    \n",
    "    else:\n",
    "        return 'OT'                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sentence_tags(sentence):\n",
    "    '''POS tag the words in the sentence and return a dict of the form : {word1 : [tag1,tag2..], word2 : [tag3,tag4..]..}'''\n",
    "    word_tag_dict = defaultdict(set)\n",
    "    #sent_tags = pos_tagger.tag(tokenize_txt(sentence))\n",
    "    sent_tags = nltk.pos_tag(tokenize_txt(sentence))\n",
    "        \n",
    "    for word_tag in sent_tags:\n",
    "        word = word_tag[0]\n",
    "        tag = word_tag[1]\n",
    "        \n",
    "        word_tag_dict[word].add(get_short_tag(tag))\n",
    "    \n",
    "    return word_tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'set'>, {u'sent': set(['VB']), u'one': set(['OT'])})\n",
      "defaultdict(<type 'set'>, {u'two': set(['OT']), u'sent': set(['NN'])})\n"
     ]
    }
   ],
   "source": [
    "print get_sentence_tags(\"sent one\")\n",
    "print get_sentence_tags(\"sent two\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_doc_tags(document):\n",
    "    \n",
    "    '''Perform POS tagging on all the sentences in the document and return a dict of the form :'''\n",
    "    ''' (sent_id : { word1 : tag1 ...}...}'''\n",
    "    \n",
    "    sent_and_tags = defaultdict(int)\n",
    "    \n",
    "    #sentences = document.split('.')\n",
    "    sentences = sent_detector.tokenize(document,realign_boundaries=True)\n",
    "    \n",
    "    for i,sentence in enumerate(sentences):\n",
    "        sent_and_tags[i] = get_sentence_tags(sentence.strip('.').strip('\\n'.strip('')))\n",
    "    \n",
    "    return sent_and_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: defaultdict(<type 'set'>, {u'Turing': set(['NN']), u'is': set(['VB']), u'Who': set(['OT']), u'?': set(['OT']), u'Alan': set(['NN'])}), 1: defaultdict(<type 'set'>, {u'Kingdom': set(['NN']), u'United': set(['NN']), u'Alan': set(['NN']), u'born': set(['VB']), u'in': set(['OT']), u'the': set(['OT']), u'was': set(['VB'])})})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_doc_tags(\"Who is Alan Turing ??. Alan was born in the United Kingdom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_tags(data_root_dir,annotation_file):\n",
    "    '''Perfom Part of Speech Tagging across all the sentences in all the documents in all the clusters'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_tags = defaultdict(defaultdict)\n",
    "    \n",
    "    i = 1\n",
    "    for clust,files in clust_files.iteritems():        \n",
    "        \n",
    "        for doc in files:\n",
    "            \n",
    "            if i %10 == 0:\n",
    "                print 'Finished tagging doc :', i\n",
    "            i += 1\n",
    "            doc_path = data_root_dir + '\\\\' + doc\n",
    "            txt = get_text_from_doc(doc_path,txt_opn_tag,txt_close_tag)\n",
    "            \n",
    "            clust_tags[clust][doc] = get_doc_tags(txt)\n",
    "            \n",
    "    return clust_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clust_tags = get_cluster_tags(data_root_dir,annotation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def serialize(file_name,data):\n",
    "    \n",
    "    with open(file_name, 'wb') as f:    \n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def deserialize(file_name):\n",
    "\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "file_name = 'pos_tags.pickle'\n",
    "#serialize(file_name,clust_tags)\n",
    "clust_tags = deserialize(file_name)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_cpy = copy.deepcopy(clust_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_pos(pos_set,pos_idx = {'NN' : 0 ,'VB' : 1,'JJ' : 2,'RB' : 3}):\n",
    "    \n",
    "    '''Convert the POS set to a binary vector according to pos_idx'''    \n",
    "    bin_pos_vec = 4*[False]\n",
    "    \n",
    "    for pos in pos_set:\n",
    "        \n",
    "        if pos == 'OT':\n",
    "            return 4*[False]\n",
    "        else:\n",
    "            bin_pos_vec[pos_idx[pos]] = True\n",
    "    \n",
    "    return bin_pos_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, False, False, True]\n",
      "[False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "print vectorize_pos({'NN','RB'})\n",
    "print vectorize_pos({'NN','RB','JJ','VB','OT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize_tags_across_clusters(clust_tags):\n",
    "    '''Binarize the POS of words'''\n",
    "\n",
    "    for clust,doc in clust_tags.iteritems(): \n",
    "\n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "\n",
    "        for doc,sent in doc.iteritems():\n",
    "\n",
    "            sent_word = defaultdict(defaultdict)\n",
    "\n",
    "            for sen_id,word_pos in sent.iteritems():\n",
    "\n",
    "\n",
    "                for word,pos in word_pos.iteritems():                            \n",
    "                    word_pos[word] = copy.deepcopy(vectorize_pos(pos))\n",
    "\n",
    "                sent_word[sen_id] = copy.deepcopy(word_pos)\n",
    "\n",
    "            doc_sent[doc] = copy.deepcopy(sent_word)\n",
    "\n",
    "        clust_tags[clust] = copy.deepcopy(doc_sent)\n",
    "\n",
    "    return clust_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_clust_tags = vectorize_tags_across_clusters(clust_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'set'>, {'and': set(['OT']), 'humans': set(['NN']), 'sales': set(['NN']), 'topic': set(['NN']), 'put': set(['VB']), 'britain': set(['NN']), 'in': set(['OT']), 'subject': set(['NN']), 'beef': set(['NN']), 'to': set(['OT']), 'crimp': set(['NN']), 'has': set(['VB']), 'be': set(['VB']), 'that': set(['OT']), 'domestic': set(['JJ']), 'pubs': set(['NN']), 'press': set(['NN']), 'a': set(['OT']), 'made': set(['VB']), 'concern': set(['NN']), 'of': set(['OT']), 'disease': set(['NN']), 'p': set(['NN']), 'transmitted': set(['VB']), 'can': set(['OT']), 'serious': set(['JJ']), 'the': set(['OT'])}) \n",
      "\n",
      "\n",
      "defaultdict(<type 'set'>, {'and': [False, False, False, False], 'humans': [True, False, False, False], 'sales': [True, False, False, False], 'topic': [True, False, False, False], 'put': [False, True, False, False], 'britain': [True, False, False, False], 'in': [False, False, False, False], 'subject': [True, False, False, False], 'beef': [True, False, False, False], 'to': [False, False, False, False], 'crimp': [True, False, False, False], 'has': [False, True, False, False], 'be': [False, True, False, False], 'that': [False, False, False, False], 'domestic': [False, False, True, False], 'pubs': [True, False, False, False], 'press': [True, False, False, False], 'a': [False, False, False, False], 'made': [False, True, False, False], 'concern': [True, False, False, False], 'of': [False, False, False, False], 'disease': [True, False, False, False], 'p': [True, False, False, False], 'transmitted': [False, True, False, False], 'can': [False, False, False, False], 'serious': [False, False, True, False], 'the': [False, False, False, False]})\n"
     ]
    }
   ],
   "source": [
    "print old_cpy['mad cow disease']['LA060490-0083'][2],'\\n\\n'\n",
    "print new_clust_tags['mad cow disease']['LA060490-0083'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 5 : A binary value equals one iff the output of the named entity classifier from CoreNLP is not empty  (Named Entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_ners(data_root_dir,annotation_file):\n",
    "    '''Perform Named Entity Recognition on all sentences in all docs in all clusters'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent,nltk_flag=True,ner_flag=True) for sent in sentences]\n",
    "            \n",
    "            sent_ner_cnt = defaultdict(int)\n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):    \n",
    "                \n",
    "                \n",
    "                ners = ner_tagger.tag(tok_sent)\n",
    "                cnt = 0\n",
    "                for ner in ners:\n",
    "                    if ner[1] != 'O':\n",
    "                        cnt += 1\n",
    "                sent_ner_cnt[s_id] = cnt\n",
    "            \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_ner_cnt)\n",
    "            \n",
    "            print 'FINISHED NER ON ', file_name\n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)\n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clust_ners = extract_ners(data_root_dir,annotation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "file_name = 'ner_tags.pickle'\n",
    "#serialize(file_name,clust_ners)\n",
    "clust_ners = deserialize(file_name)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 1, 1: 1, 2: 1, 3: 0, 4: 0, 5: 0, 6: 1, 7: 0, 8: 1, 9: 1, 10: 0, 11: 2, 12: 1, 13: 0, 14: 1, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 2, 24: 0, 25: 0, 26: 0, 27: 0, 28: 1, 29: 0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_ners['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 6 : A binary value denotes if a word in Number  (Number)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_digit_cnt(data_root_dir,annotation_file,cnt_ratio_flag='C'):\n",
    "    '''Count the number of digits in a sentence'''\n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent) for sent in sentences]\n",
    "            \n",
    "            sent_dig_cnt = defaultdict(int)\n",
    "            \n",
    "            dig_cnt = 0\n",
    "            for s_id,tok_sent in enumerate(sent_tokens):                                    \n",
    "                for tok in tok_sent:\n",
    "                    if tok.isdigit():\n",
    "                        dig_cnt += 1\n",
    "                if cnt_ratio_flag == 'C':\n",
    "                    sent_dig_cnt[s_id] = dig_cnt\n",
    "                else:\n",
    "                    sent_dig_cnt[s_id] = float(dig_cnt)/len(tok_sent)\n",
    "        \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_dig_cnt)            \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_digs = extract_digit_cnt(data_root_dir,annotation_file)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print clust_digs['mad cow disease']['LA060490-0083'][29]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 22 : The number of digits, divided by the sentence length(Number ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.03225806451612903, 9: 0.2, 10: 0.06451612903225806, 11: 0.10526315789473684, 12: 0.08333333333333333, 13: 0.07142857142857142, 14: 0.1111111111111111, 15: 0.2, 16: 0.09090909090909091, 17: 0.2222222222222222, 18: 0.07142857142857142, 19: 0.06666666666666667, 20: 0.10714285714285714, 21: 0.125, 22: 1.0, 23: 0.10714285714285714, 24: 0.1875, 25: 0.12, 26: 0.3333333333333333, 27: 0.21428571428571427, 28: 0.10714285714285714, 29: 1.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_dig_ratio = extract_digit_cnt(data_root_dir,annotation_file,'R')\n",
    "print 'done'\n",
    "clust_dig_ratio['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 23 : The number of stop words, divided by the sentence length(Stop word ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stop_word_ratio(data_root_dir,annotation_file):\n",
    "    '''Compute the stop word ratio for all sentences'''\n",
    "    '''stop word ratio == no of stop words in sent / len(sent) '''\n",
    "    \n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent) for sent in sentences]\n",
    "            \n",
    "            sent_dig_cnt = defaultdict(int)\n",
    "            \n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):      \n",
    "                stop_cnt = 0\n",
    "                for tok in tok_sent:\n",
    "                    if tok.lower() in english_stopwords:\n",
    "                        stop_cnt += 1\n",
    "                sent_dig_cnt[s_id] = float(stop_cnt)/len(tok_sent)\n",
    "        \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_dig_cnt)            \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "0.392857142857\n"
     ]
    }
   ],
   "source": [
    "clust_stop_word_ratio = stop_word_ratio(data_root_dir,annotation_file)\n",
    "print 'done'\n",
    "print clust_stop_word_ratio['mad cow disease']['LA060490-0083'][18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 24 : No of words in the sentence (Sentence Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_length(data_root_dir,annotation_file):\n",
    "    '''Compute the Lenght of sentences and store them in a dictionary'''    \n",
    "        \n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent) for sent in sentences]\n",
    "            \n",
    "            sent_dig_cnt = defaultdict(int)\n",
    "            \n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):      \n",
    "                sent_dig_cnt[s_id] = len(tok_sent)\n",
    "        \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_dig_cnt)            \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "clust_sent_lens = sent_length(data_root_dir,annotation_file)\n",
    "print 'done'\n",
    "print clust_sent_lens['mad cow disease']['LA060490-0083'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "file_path = data_root_dir + '\\\\' + 'LA060490-0083'\n",
    "doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "sentences = sent_detector.tokenize(doc)\n",
    "print len(sentences[15].split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 21 : The number of named entities divided by sentence length (NER Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ner_ratio(data_root_dir,annotation_file,clust_ners,clust_sent_lens):\n",
    "    '''Compute the Ratio of NERS : Sentence lenght and store them in a dictionary'''            \n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            total_sents = len(sent_detector.tokenize(doc))\n",
    "            #sent_tokens =[tokenize_txt(sent) for sent in sentences]\n",
    "            \n",
    "            sent_ner_ratio = defaultdict(int)\n",
    "            \n",
    "            \n",
    "            for i in range(0,total_sents):\n",
    "                sent_ner_ratio[i] = float(clust_ners[clust][file_name][i])/clust_sent_lens[clust][file_name][i]\n",
    "                        \n",
    "        \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_ner_ratio)            \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "0.105263157895\n"
     ]
    }
   ],
   "source": [
    "clust_ner_ratio = ner_ratio(data_root_dir,annotation_file,clust_ners,clust_sent_lens)\n",
    "print 'done'\n",
    "print clust_ner_ratio['mad cow disease']['LA060490-0083'][11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 20 : The number of nouns,verbs,adverbs, adjectives in the sentence, divided by the length of the sentence (POS Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_ratio(data_root_dir,annotation_file,new_clust_tags,clust_sent_lens):\n",
    "    '''Compute the Ratio of nouns,verbs,adverbs and adjectives : Sentence lenght and store them in a dictionary'''\n",
    "      \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    clusters = clust_sent_lens.keys()      \n",
    "    for clust in clusters: \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        files = clust_sent_lens[clust].keys()\n",
    "                \n",
    "        for _file in files:\n",
    "            sent_ids = clust_sent_lens[clust][_file].keys()\n",
    "            \n",
    "            sent_pos_ratio = defaultdict(int)\n",
    "            \n",
    "                \n",
    "            for sent_id in sent_ids:\n",
    "                pos_cnt = 0\n",
    "                for word,tag_lst in new_clust_tags[clust][_file][sent_id].iteritems():\n",
    "                    '''\n",
    "                    if _file == 'LA060490-0083' and sent_id == 3:\n",
    "                        print tag_lst, pos_cnt*1.0/clust_sent_lens[clust][_file][sent_id]\n",
    "                        #print new_clust_tags[clust][_file][sent_id]                    \n",
    "                    '''\n",
    "                    if True in tag_lst:\n",
    "                        pos_cnt += 1                \n",
    "                sent_pos_ratio[sent_id] = float(pos_cnt)/ clust_sent_lens[clust][_file][sent_id]\n",
    "                \n",
    "            doc_sent[_file] = copy.deepcopy(sent_pos_ratio)   \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "    \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_pos_ratios = pos_ratio(data_root_dir,annotation_file,new_clust_tags,clust_sent_lens)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5263157894736842"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_pos_ratios['mad cow disease']['LA060490-0083'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 14 : The position of the sentence. Suppose there are M sentences in the document \n",
    "    , then for the ith sentence the position is computed as 1-(i-1)/(M-1)              (POSITION)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_pos(data_root_dir,annotation_file,clust_sent_lens):\n",
    "    '''Compute the position of the sentence, according to above formula'''\n",
    "      \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    clusters = clust_sent_lens.keys()      \n",
    "    for clust in clusters: \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        files = clust_sent_lens[clust].keys()\n",
    "                \n",
    "        for _file in files:\n",
    "            sent_ids = clust_sent_lens[clust][_file].keys()\n",
    "            \n",
    "            total_sents = len(clust_sent_lens[clust][_file].keys())\n",
    "            \n",
    "            #Avoid divide by 0 error\n",
    "            if total_sents == 1:\n",
    "                total_sents = 2 \n",
    "                \n",
    "            sent_positon = defaultdict(int)\n",
    "            \n",
    "                \n",
    "            for sent_id in sent_ids:\n",
    "                sent_positon[sent_id] =  1 - ( float( sent_id ) / (total_sents - 1) )\n",
    "                \n",
    "            doc_sent[_file] = copy.deepcopy(sent_positon)   \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "    \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_sent_pos = sentence_pos(data_root_dir,annotation_file,clust_sent_lens)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 1.0, 1: 0.9655172413793104, 2: 0.9310344827586207, 3: 0.896551724137931, 4: 0.8620689655172413, 5: 0.8275862068965517, 6: 0.7931034482758621, 7: 0.7586206896551724, 8: 0.7241379310344828, 9: 0.6896551724137931, 10: 0.6551724137931034, 11: 0.6206896551724138, 12: 0.5862068965517242, 13: 0.5517241379310345, 14: 0.5172413793103448, 15: 0.48275862068965514, 16: 0.4482758620689655, 17: 0.4137931034482759, 18: 0.3793103448275862, 19: 0.3448275862068966, 20: 0.31034482758620685, 21: 0.27586206896551724, 22: 0.24137931034482762, 23: 0.2068965517241379, 24: 0.1724137931034483, 25: 0.13793103448275867, 26: 0.10344827586206895, 27: 0.06896551724137934, 28: 0.03448275862068961, 29: 0.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_sent_pos['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 17 : The mean TF of all words in the sentence, divided by the sentence length (Averaged TF)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def averaged_tf(data_root_dir,annotation_file,clust_word_tfs):\n",
    "    '''Get the average TF values of words in a sentence and them in a dictionary'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:                        \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent,nltk_flag=True,ner_flag=True) for sent in sentences]\n",
    "            \n",
    "            sent_mean_tf = defaultdict(int)\n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):    \n",
    "                mean_tf = 0\n",
    "                for word in tok_sent:\n",
    "                    mean_tf += clust_word_tfs[clust][word]\n",
    "                mean_tf = float(mean_tf)/len(tok_sent)\n",
    "                \n",
    "                sent_mean_tf[s_id] = mean_tf\n",
    "            \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_mean_tf)\n",
    "                        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)\n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_mean_tfs = averaged_tf(data_root_dir,annotation_file,clust_word_tfs)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 52.46875, 1: 33.78787878787879, 2: 79.97222222222223, 3: 67.77777777777777, 4: 54.36363636363637, 5: 120.4375, 6: 39.529411764705884, 7: 42.21739130434783, 8: 27.208333333333332, 9: 6.5, 10: 46.81481481481482, 11: 67.47368421052632, 12: 39.476190476190474, 13: 62.72, 14: 33.5625, 15: 50.09090909090909, 16: 59.388888888888886, 17: 60.22222222222222, 18: 74.3103448275862, 19: 26.076923076923077, 20: 62.21739130434783, 21: 59.8, 22: 0.6666666666666666, 23: 44.31818181818182, 24: 71.92857142857143, 25: 32.57142857142857, 26: 10.333333333333334, 27: 49.63636363636363, 28: 45.65384615384615, 29: 0.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_mean_tfs['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 18 : The mean IDF of all words in the sentence, divided by the sentence length (Averaged IDF)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def averaged_idf(data_root_dir,annotation_file,doc_freqs):\n",
    "    '''Get the average IDF values of words in a sentence and them in a dictionary'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:                        \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent,nltk_flag=True,ner_flag=True) for sent in sentences]\n",
    "            \n",
    "            sent_mean_idf = defaultdict(int)\n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):    \n",
    "                mean_idf = 0\n",
    "                for word in tok_sent:\n",
    "                    mean_idf += doc_freqs[word]\n",
    "                mean_idf = float(mean_idf)/len(tok_sent)\n",
    "                \n",
    "                sent_mean_idf[s_id] = mean_idf\n",
    "            \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_mean_idf)\n",
    "                        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)\n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_mean_idfs = averaged_idf(data_root_dir,annotation_file,doc_freqs)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 32.9676003655945, 1: 33.900159108011366, 2: 36.70248740133245, 3: 22.232825952937723, 4: 20.81086023515057, 5: 16.980816560429865, 6: 17.692625701242967, 7: 26.943353899985787, 8: 28.281419427609936, 9: 9.258651554752458, 10: 18.860165464137836, 11: 52.797447968030994, 12: 34.1233555537647, 13: 19.791633880215812, 14: 29.56881183615931, 15: 11.795619061560934, 16: 27.671832240526257, 17: 30.09017123117265, 18: 11.606496726037758, 19: 21.9815518896864, 20: 30.171094131562324, 21: 25.695866018971874, 22: 137.33333333333334, 23: 21.31402431675951, 24: 35.044368860474044, 25: 32.764443042707775, 26: 27.279400223496722, 27: 40.18385425177738, 28: 16.169330035486237, 29: 51.5})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_mean_idfs['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 19 : The mean CF of all words in the sentence, divided by the sentence length (Averaged CF)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def averaged_cf(data_root_dir,annotation_file,clust_dfs):\n",
    "    '''Get the average Cluster freqs values of words in a sentence and them in a dictionary'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:                        \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent,nltk_flag=True,ner_flag=True) for sent in sentences]\n",
    "            \n",
    "            sent_mean_cf = defaultdict(int)\n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):    \n",
    "                mean_cf = 0\n",
    "                for word in tok_sent:\n",
    "                    mean_cf += clust_dfs[clust][word]\n",
    "                mean_cf = float(mean_cf)/len(tok_sent)\n",
    "                \n",
    "                sent_mean_cf[s_id] = mean_cf\n",
    "            \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_mean_cf)\n",
    "                        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)\n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_mean_cfs = averaged_cf(data_root_dir,annotation_file,clust_dfs)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 4.9375, 1: 4.121212121212121, 2: 5.694444444444445, 3: 5.0, 4: 3.9545454545454546, 5: 6.125, 6: 3.0588235294117645, 7: 3.4782608695652173, 8: 3.6666666666666665, 9: 2.625, 10: 4.333333333333333, 11: 4.684210526315789, 12: 3.1904761904761907, 13: 5.56, 14: 4.3125, 15: 3.727272727272727, 16: 5.222222222222222, 17: 5.111111111111111, 18: 6.586206896551724, 19: 4.5, 20: 5.260869565217392, 21: 5.0, 22: 0.6666666666666666, 23: 4.2272727272727275, 24: 4.214285714285714, 25: 3.9523809523809526, 26: 3.6666666666666665, 27: 3.3636363636363638, 28: 4.576923076923077, 29: 0.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_mean_cfs['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_rouge_n_score(sent_1,sent_2,n=2,do_stem=True):\n",
    "    '''Normalize the overlapping n-grams and return the score'''\n",
    "    '''Sentences are converted to lower-case and words are stemmed'''\n",
    "    \n",
    "    #lower\n",
    "    sent_1 = sent_1.lower()\n",
    "    sent_2 = sent_2.lower()\n",
    "    \n",
    "    \n",
    "    tokenizer = tokenizers.Tokenizer('ENGLISH')\n",
    "\n",
    "    sent_1_toks = tokenizer.to_words(sent_1)\n",
    "    sent_2_toks = tokenizer.to_words(sent_2)\n",
    "    \n",
    "    \n",
    "    #stem the sentence\n",
    "    if do_stem == True:\n",
    "        sent_1 = ' '.join([stem(tok) for tok in sent_1_toks])\n",
    "        sent_2 = ' '.join([stem(tok) for tok in sent_2_toks])\n",
    "    \n",
    "    \n",
    "    sent_obj_1= dom.Sentence(sent_1,tokenizer)\n",
    "    sent_obj_2= dom.Sentence(sent_2,tokenizer)\n",
    "    \n",
    "    \n",
    "    return evaluation.rouge_n([sent_obj_1],[sent_obj_2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROGUE With stemming:  1.0\n",
      "ROGuE Without stemming:  0.666666666667\n"
     ]
    }
   ],
   "source": [
    "print 'ROGUE With stemming: ' , get_rouge_n_score('This iS SentENce CooLing','This is Sentence cool',2,True)\n",
    "print 'ROGuE Without stemming: ' , get_rouge_n_score('This iS SentENce CooLing','This is Sentence cool',2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_docs_without_summary(data_root_dir,annotation_file):\n",
    "    '''Return a dictionary of the form {clust1 : [doc1,doc2...],clust2 : [doc1,doc2...] ....}'''\n",
    "    '''The key is the cluster name, the value is a list of documents, for which summary do not exist'''\n",
    "    '''This is because, certain documents in the DUC dataset do not have a summary. To weed out such documents, this function\n",
    "       will be called.'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    files_with_summ = set( [fname.lower() for fname in listdir(data_root_dir+ '\\\\' + 'Summaries' + '\\\\')] )\n",
    "    clust_docs_wo_summ = defaultdict(list)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():\n",
    "        for _file in files:\n",
    "            tmp = _file + '.txt'\n",
    "            if tmp.lower() not in files_with_summ:\n",
    "                clust_docs_wo_summ[clust].append(_file)\n",
    "    \n",
    "    return clust_docs_wo_summ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'list'>, {'ben johnson': ['AP880928-0054'], 'tornadoes': ['AP891116-0191', 'LA070190-0073'], 'crash': ['FBIS4-4674'], 'illegal aliens': ['LA071589-0076'], 'u.s. tanker spill': ['AP830325-0143'], 'north american free trade agreement': ['FT934-10911']})\n"
     ]
    }
   ],
   "source": [
    "docs_without_summ = get_docs_without_summary(data_root_dir,annotation_file)\n",
    "print docs_without_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_gold_summ_from_doc(document_path):\n",
    "    '''Extract the Gold summary of a document.'''\n",
    "    '''Gold summary is of the form <Abstract:> This is the summary <Introduction:>'''\n",
    "    start_tag = 'Abstract:'\n",
    "    close_tag = 'Introduction:'\n",
    "    \n",
    "    f = open(document_path,'r')\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    start = content.index(start_tag) + len(start_tag)\n",
    "    end   = content.index(close_tag)\n",
    "    \n",
    "    return content[start:end].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lawmakers are having difficulty coming to an agreement on whether or not illegal aliens should be counted in the 1990 census. The U.S. Constitution requires the Census Bureau to count all the \"persons\" in the country every ten years for purposes of reapportionment of seats in the house of representatives. It does not specify citizens. Some representatives feel that illegal aliens, if counted, may give some states like California and Texas and extra seat. Since there is a 435-seat limit, some representatives fear that their states may loose a seat. In the past, every state has counted legal and illegal aliens in the census.\n"
     ]
    }
   ],
   "source": [
    "doc_path = data_root_dir+ '\\\\' + 'Summaries' + '\\\\' + 'ap880623-0135.txt'\n",
    "summ = extract_gold_summ_from_doc(doc_path)\n",
    "print summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Construct the train Matrix with the dimensions M*N,where: \n",
    "<b>N = $\\sum_{i=1}^c\\sum_{j=1}^{d_i}{X_{ij}}$ where c is the no of clusters, ${d_i}$ is the no of docs in ${cluster_i}$ , $X_{ij}$ is the no of sentences in $j^{th}$ doc of $i^{th} cluster$<b><br>\n",
    "<b>M = no of features for every sentence</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_dict_to_feature_column(clust_files,docs_without_summ):\n",
    "    '''Convert the nested dictionary to a feature column'''    \n",
    "    feature_col = []\n",
    "    \n",
    "    clusters = sorted(clust_files.keys())\n",
    "    \n",
    "    for clust in clusters:         \n",
    "        \n",
    "        files = sorted(clust_files[clust].keys())\n",
    "                \n",
    "        for _file in files:\n",
    "            \n",
    "            #Ignore the docs that do not have a summary.            \n",
    "            if _file not in docs_without_summ[clust]:\n",
    "                sent_ids = sorted(clust_files[clust][_file].keys())                            \n",
    "                for sent_id in sent_ids:\n",
    "                    feature_col.append(clust_files[clust][_file][sent_id])            \n",
    "\n",
    "    \n",
    "    return np.array(feature_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_X_Matrix(clust_sent_pos,clust_sent_lens,clust_mean_tfs,clust_mean_idfs,clust_mean_cfs,clust_pos_ratios,\n",
    "                       clust_ner_ratio,clust_dig_ratio,clust_stop_word_ratio):\n",
    "    \n",
    "    '''Construct the X_Matrix by stacking the Features, columnwise, for all sentences. Finally return X_train'''\n",
    "    \n",
    "    F_position        = convert_dict_to_feature_column(clust_sent_pos,docs_without_summ)\n",
    "    F_length          = convert_dict_to_feature_column(clust_sent_lens,docs_without_summ)\n",
    "    F_mean_tfs        = convert_dict_to_feature_column(clust_mean_tfs,docs_without_summ)\n",
    "    F_mean_idfs       = convert_dict_to_feature_column(clust_mean_idfs,docs_without_summ)\n",
    "    F_mean_cfs        = convert_dict_to_feature_column(clust_mean_cfs,docs_without_summ)\n",
    "    F_pos_ratio       = convert_dict_to_feature_column(clust_pos_ratios,docs_without_summ)\n",
    "    F_ner_ratio       = convert_dict_to_feature_column(clust_ner_ratio,docs_without_summ)\n",
    "    F_dig_ratio       = convert_dict_to_feature_column(clust_dig_ratio,docs_without_summ)\n",
    "    F_stop_word_ratio = convert_dict_to_feature_column(clust_stop_word_ratio,docs_without_summ)\n",
    "    \n",
    "    stack = (F_position,F_length,F_mean_tfs,F_mean_idfs,F_mean_cfs,F_pos_ratio,F_ner_ratio,F_dig_ratio,F_stop_word_ratio)\n",
    "    return np.column_stack(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_Y(clust_files,docs_without_summ):\n",
    "    '''Construct the Y output value(ROGUE Score) for every sentence in the document, along\n",
    "       with the gold summary of the document.  i.e ROGUE(sentence,summary)'''    \n",
    "    feature_col = []\n",
    "    \n",
    "    clusters = sorted(clust_files.keys())\n",
    "    \n",
    "    for clust in clusters:         \n",
    "        \n",
    "        files = sorted(clust_files[clust])\n",
    "                \n",
    "        for _file in files:\n",
    "            \n",
    "            #Ignore the docs that do not have a summary.            \n",
    "            if _file not in docs_without_summ[clust]:\n",
    "\n",
    "                file_path = data_root_dir + '\\\\' + _file\n",
    "                doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "                sentences = sent_detector.tokenize(doc)\n",
    "                \n",
    "                sum_file_path = data_root_dir+ '\\\\' + 'Summaries' + '\\\\' + _file + '.txt'\n",
    "                gold_summ = extract_gold_summ_from_doc(sum_file_path)\n",
    "                \n",
    "                for sent in sentences:\n",
    "                    try:\n",
    "                        rouge_score = get_rouge_n_score(sent,gold_summ)                        \n",
    "                    except:\n",
    "                        rouge_score = 0\n",
    "                    \n",
    "                    #To avoid divide by zero error\n",
    "                    feature_col.append(rouge_score)\n",
    "                        \n",
    "    \n",
    "    return np.array(feature_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Cross Validate and plot the predicted mean ROGUE square error</b><br><br>\n",
    "The Cost function is $J(\\theta)=\\frac{1}{m}\\sum_{i=1}^m ROGUE\\_SCORE(pred_i,actual_i),$ <br>where \n",
    "$pred_i $ is the predicted ROGUE score of $sentence_i$ and $actual_i $ is the real ROGUE score of $sentence_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_cross_validation(X_Matrix,Y,clf,n_folds,degree):\n",
    "    '''Perform n-fold cross validation.    \n",
    "    Params:\n",
    "        X.........a Matrix of features\n",
    "        y.........the true rogue score of each sentence\n",
    "        n_folds...the number of folds of cross-validation to do\n",
    "        \n",
    "    Return:\n",
    "        the average testing accuracy across all folds.'''\n",
    "    \n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_Matrix = poly.fit_transform(X_Matrix)\n",
    "    \n",
    "    accuracies = []\n",
    "    cv = KFold(len(Y), n_folds)\n",
    "    for train_idx, test_idx in cv:        \n",
    "        clf.fit(X_Matrix[train_idx], Y[train_idx])\n",
    "        predicted = clf.predict(X_Matrix[test_idx])        \n",
    "        error = np.mean(np.abs(predicted - Y[test_idx]))\n",
    "        accuracies.append(error)                \n",
    "    avg = np.mean(accuracies)    \n",
    "    return avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_accuracies(X_Matrix,Y,clf,n_folds=10,poly_degrees=[1,2,3,4]):\n",
    "    '''Plot a graph of Test Error vs Polynomial Order'''        \n",
    "    errors = [do_cross_validation(X_Matrix,Y,clf,n_folds,degree) for degree in poly_degrees]            \n",
    "    plt.ylabel('Test Error')\n",
    "    plt.xlabel('Polynomial Degree')\n",
    "    plt.plot(poly_degrees, errors,'r-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_best_order(n_folds,Y,poly_degrees):    \n",
    "    '''Experiment with various settings and figure the best polynomial setting'''    \n",
    "    X_Matrix = construct_X_Matrix(clust_sent_pos,clust_sent_lens,clust_mean_tfs,clust_mean_idfs,clust_mean_cfs,clust_pos_ratios,\n",
    "                           clust_ner_ratio,clust_dig_ratio,clust_stop_word_ratio)\n",
    "    \n",
    "    \n",
    "    plot_accuracies(X_Matrix,Y,Ridge(),n_folds,poly_degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEPCAYAAACQmrmQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n51xiPMk455xLjCcZ55xzifl/ZjTpAld0h2cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf11dba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y = construct_Y(clust_files,docs_without_summ)\n",
    "n_folds=10\n",
    "poly_degrees=[1,2,3]\n",
    "find_best_order(n_folds,Y,poly_degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Seems the Validation error is minimum when the polynomial order is 2. Raise the X_Matrix to this order and fit the Regressor</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_clf(best_order,Y,clf):\n",
    "    \n",
    "    poly = PolynomialFeatures(best_order)        \n",
    "    \n",
    "    X_Matrix = construct_X_Matrix(clust_sent_pos,clust_sent_lens,clust_mean_tfs,clust_mean_idfs,clust_mean_cfs,clust_pos_ratios,\n",
    "                           clust_ner_ratio,clust_dig_ratio,clust_stop_word_ratio)\n",
    "    X_Matrix = poly.fit_transform(X_Matrix)\n",
    "    \n",
    "    print X_Matrix\n",
    "    clf.fit(X_Matrix,Y)\n",
    "    print '\\nFitted Regressor with best settings'\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e+00   1.00000000e+00   3.20000000e+01 ...,   0.00000000e+00\n",
      "    0.00000000e+00   1.18164062e-01]\n",
      " [  1.00000000e+00   9.78260870e-01   2.50000000e+01 ...,   0.00000000e+00\n",
      "    0.00000000e+00   1.93600000e-01]\n",
      " [  1.00000000e+00   9.56521739e-01   2.80000000e+01 ...,   1.27551020e-03\n",
      "    1.27551020e-02   1.27551020e-01]\n",
      " ..., \n",
      " [  1.00000000e+00   3.44827586e-02   2.40000000e+01 ...,   2.77777778e-02\n",
      "    7.63888889e-02   2.10069444e-01]\n",
      " [  1.00000000e+00   1.72413793e-02   3.40000000e+01 ...,   1.38408304e-02\n",
      "    2.76816609e-02   5.53633218e-02]\n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+01 ...,   1.60000000e-01\n",
      "    1.20000000e-01   9.00000000e-02]]\n",
      "\n",
      "Fitted Regressor with best settings\n"
     ]
    }
   ],
   "source": [
    "clf = get_best_clf(2,Y,Ridge())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_X_Matrix_for_test_doc(cluster,document,clust_sent_pos,clust_sent_lens,clust_mean_tfs,clust_mean_idfs,\n",
    "                                    clust_mean_cfs,clust_pos_ratios,clust_ner_ratio,clust_dig_ratio,\n",
    "                                    clust_stop_word_ratio,poly_order):\n",
    "    \n",
    "    '''Extract all the features for a given document and return the extracted features'''\n",
    "    \n",
    "    X_Matrix = []\n",
    "    \n",
    "    for sent_id in clust_sent_pos[cluster][document].keys():                \n",
    "        \n",
    "        F_position = clust_sent_pos[cluster][document][sent_id]\n",
    "        F_length = clust_sent_lens[cluster][document][sent_id]\n",
    "        F_mean_tfs = clust_mean_tfs[cluster][document][sent_id]\n",
    "        F_mean_idfs = clust_mean_idfs[cluster][document][sent_id]\n",
    "        F_mean_cfs = clust_sent_pos[cluster][document][sent_id]\n",
    "        F_pos_ratio = clust_pos_ratios[cluster][document][sent_id]\n",
    "        F_ner_ratio = clust_ner_ratio[cluster][document][sent_id]\n",
    "        F_dig_ratio = clust_dig_ratio[cluster][document][sent_id]\n",
    "        F_stop_word_ratio = clust_stop_word_ratio[cluster][document][sent_id]\n",
    "        \n",
    "        row = [F_position,F_length,F_mean_tfs,F_mean_idfs,F_mean_cfs,\n",
    "                        F_pos_ratio,F_ner_ratio,F_dig_ratio,F_stop_word_ratio]\n",
    "        \n",
    "        X_Matrix.append(row)\n",
    "    \n",
    "    poly = PolynomialFeatures(poly_order)\n",
    "    X_Matrix = poly.fit_transform(np.array(X_Matrix))\n",
    "    \n",
    "    return X_Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Greedy Based Sentence Selection.<br>\n",
    "-are_sentences_salient()<br>\n",
    "-select_sentences()<br></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def are_sentences_salient(clust,sent_1,sent_2,threshold=0.6):\n",
    "    '''Check if the sentences are salient based on a threshold.\n",
    "       If COSINE_SIM(sent_1,sent_2) < 0.6, return True. Else False'''\n",
    "    \n",
    "    sent_1_toks = tokenize_txt(sent_1)\n",
    "    sent_2_toks = tokenize_txt(sent_2)\n",
    "    \n",
    "    vocab = list(set(sent_1_toks) | set(sent_2_toks))\n",
    "    \n",
    "    vec_1 = []\n",
    "    vec_2 = []\n",
    "    \n",
    "    for token in vocab:                        \n",
    "        tf = clust_word_tfs[clust][token]\n",
    "        idf = doc_freqs[token]                \n",
    "        tf_idf = tf*idf\n",
    "                \n",
    "        if token in sent_1_toks and token in sent_2_toks:            \n",
    "            vec_1.append(tf_idf)\n",
    "            vec_2.append(tf_idf)            \n",
    "        elif token in sent_1_toks and token not in sent_2_toks:            \n",
    "            vec_1.append(tf_idf)\n",
    "            vec_2.append(0.0)\n",
    "        elif token not in sent_1_toks and token in sent_2_toks:            \n",
    "            vec_1.append(0.0)\n",
    "            vec_2.append(tf_idf)\n",
    "    \n",
    "    vec_1 = np.array(vec_1).reshape(1,-1)\n",
    "    vec_2 = np.array(vec_2).reshape(1,-1)\n",
    "    \n",
    "    sim_score = list(cosine_similarity(vec_1,vec_2)[0])[0]        \n",
    "    \n",
    "    if sim_score < threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "s1 = 'A commission chaired by Professor Sir Richard Southwood '\n",
    "s2 = 'The government has committed $19 million to finding the cause of the disease Germany'\n",
    "print are_sentences_salient('mad cow disease',s1  ,s2,threshold=0.6)\n",
    "\n",
    "s1 = '$19 million disease Germany '\n",
    "s2 = 'The government has committed $19 million to finding the cause of the disease Germany'\n",
    "print are_sentences_salient('mad cow disease',s1  ,s2,threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doc_to_sent_list(document):\n",
    "    '''Convert a document to a list of sentences'''\n",
    "    \n",
    "    file_path = data_root_dir + '\\\\' + document\n",
    "    doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "    sentences = sent_detector.tokenize(doc)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_sentences(cluster,document,y_hats,sents_in_summ):\n",
    "    '''In each step of the selection,based on the greedy approach, select a sentence if it satisfies 2 conditions:\n",
    "       1. It has the next maximum predicted ROGUE score\n",
    "       2. It is salient and not very similar to the previously generated sentence in the summary'''\n",
    "       \n",
    "    top_scores = sorted(y_hats,reverse=True)\n",
    "    \n",
    "    prev_sent = ''\n",
    "    sent_id = 0\n",
    "    j = 0\n",
    "    \n",
    "    all_sentences = doc_to_sent_list(document)\n",
    "    \n",
    "    summary = ''\n",
    "    while(sent_id < sents_in_summ and j < len(top_scores)):\n",
    "        top_sent_idx = y_hats.index(top_scores[j])\n",
    "        cur_sent = all_sentences[top_sent_idx]\n",
    "        if are_sentences_salient(cluster,prev_sent,cur_sent,threshold=0.6):\n",
    "            summary += cur_sent + ' '\n",
    "            prev_sent = cur_sent\n",
    "            sent_id += 1            \n",
    "        j += 1\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_summary(cluster,document,clf,order,i,sents_in_summ=2):\n",
    "    \n",
    "    '''Generate the summary for a document with sents_in_summ number of sentences in it'''\n",
    "    \n",
    "    X_Matrix = construct_X_Matrix_for_test_doc(cluster,document,clust_sent_pos,clust_sent_lens,clust_mean_tfs,\n",
    "                                clust_mean_idfs,clust_mean_cfs,clust_pos_ratios,clust_ner_ratio,\n",
    "                                clust_dig_ratio,clust_stop_word_ratio,order)\n",
    "    \n",
    "    y_hats = list(clf.predict(X_Matrix))   \n",
    "    \n",
    "    print 'Generated SUMMARY for doc ',i, '::\\n-----------------------------------'        \n",
    "    summary = select_sentences(cluster,document,y_hats,sents_in_summ)\n",
    "    print str.replace(str.replace(summary,'<P>',''),'</P>','').strip()\n",
    "    print '\\n'        \n",
    "    \n",
    "    print 'Actual SUMMARY for doc ',i, '::\\n-----------------------------------'    \n",
    "    summary_path = data_root_dir+ '\\\\' + 'Summaries' + '\\\\' + document + '.txt'\n",
    "    print extract_gold_summ_from_doc(summary_path)\n",
    "    print '\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'\n",
    "    \n",
    "    '''\n",
    "    print 'COMPLETE TEXT :: \\n---------------------'\n",
    "    complete_text = ''.join([sent for sent in sentences])\n",
    "    print str.replace(str.replace(complete_text,'<P>',''),'</P>','').strip()\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_summaries(prob,sents=1):\n",
    "    '''Generate summaries for approx ( (1-prob) *total_docs) number of documents\n",
    "       total_docs = 300 approx.\n",
    "       prob should be in the range : 0 <= prob <= 1.0 . 0 indicates all documents, 1 indicates none of the documents'''\n",
    "    \n",
    "    i = 1\n",
    "    for clust,docs in clust_files.items():\n",
    "        for doc in docs:        \n",
    "            if np.random.uniform(low=0.0, high=1.0) > prob and doc not in docs_without_summ[clust] :\n",
    "                generate_summary(clust,doc,clf,2,i,sents)\n",
    "                i += 1\n",
    "                \n",
    "    print 'Generation Complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SUMMARY for doc  1 ::\n",
      "-----------------------------------\n",
      "DAMAGE CAUSED by Hurricane Andrew could rise to Dollars 20bn, it was\n",
      "estimated yesterday, as one of the costliest US storms this century\n",
      "threatened a further devastating landfall near the city of New Orleans.\n",
      "\n",
      "\n",
      "Actual SUMMARY for doc  1 ::\n",
      "-----------------------------------\n",
      "As hurricane Andrew headed west along the coast of the Gulf of Mexico, government officials in Louisiana, Mississippi and Texas yesterday urged more than two million people to evacuate coastal areas. The likeliest landfall is to the west of New Orleans, a city particularly vulnerable to flooding since it lies below sea level, has the Mississippi River running through it, and has a large lake immediately to its north. In Florida, Andrew hit hardest 10-15 miles south of Miami, leveling the town of Homestead and a nearby air force base. It was estimated today that damage from Andrew could reach $20 billion.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Generated SUMMARY for doc  2 ::\n",
      "-----------------------------------\n",
      "Exxon Corp. on Wednesday increased its estimate of the total 1989 costs of \n",
      "cleaning up the massive Alaskan oil spill to $2 billion and said it would take \n",
      "another $500-million charge in the fourth quarter to cover costs from what is \n",
      "now the most expensive environmental disaster in history.\n",
      "\n",
      "\n",
      "Actual SUMMARY for doc  2 ::\n",
      "-----------------------------------\n",
      "Exxon increased its estimate of the 1989 costs of cleaning up the most expensive environmental disaster in history to $2B and it could take another $500M. Exxon noted that these costs were the major reason for their lower net income for the year, but analysts said that Exxon's size would insulate it from serious long-term financial damage. Exxon's stock has lagged behind that of major oil firms and Exxon could suffer from continuing negative publicity from the Valdez spill and the subsequent heating oil spill into a waterway between New York and New Jersey.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Generated SUMMARY for doc  3 ::\n",
      "-----------------------------------\n",
      "People caught by Hurricane Hugo last\n",
      "year might disagree, but forecasters here say the deadly storm may\n",
      "have had a positive side effect _ it got the public's attention.\n",
      "\n",
      "\n",
      "Actual SUMMARY for doc  3 ::\n",
      "-----------------------------------\n",
      "Hurricane Hugo got the public's attention. It caused an unprecedented $10B in damage, killed 28 people in the Lesser Antilles, and 29 in South Carolina. But it could have been more deadly if it hit almost any place else. Early warnings about Hugo allowed 350,000 people to be evacuated safely. Hugo was the worst hurricane to strike the southeastern US coast since Betsy hit Florida in 1965, killing 74. Since then, the population of areas such as south Florida has ballooned and most residents have never directly experienced a hurricane. Hugo may have been the first in a new era of killer storms.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Generated SUMMARY for doc  4 ::\n",
      "-----------------------------------\n",
      ";   \n",
      "Hispanics who have strong American Indian ancestry may be vulnerable to\n",
      "diabetes because about 50 percent of all Indians develop diabetes, McNamara\n",
      "said.\n",
      "\n",
      "\n",
      "Actual SUMMARY for doc  4 ::\n",
      "-----------------------------------\n",
      "Hispanics in the United States are two to three times more likely to develop diabetes than the general population. Heredity and diet seem to be important factors in a disease that affects 14- 21% of Hispanics and up to 40% of those 64 to 75 years of age, compared to 7% of all Americans. Heredity, a high fat diet and obesity are predisposing factors, especially in those with American Indian ancestry. Management includes diet, exercise and medication. The St. Louis Health Center in Morgan Hill, California is sponsoring a diabetes management program for Hispanic diabetics.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Generated SUMMARY for doc  5 ::\n",
      "-----------------------------------\n",
      "``You eliminate all those causes, you're down to arson,'' said\n",
      "Dale Smallwood, a criminal investigator for Missouri's Mark Twain\n",
      "National Forest.\n",
      "\n",
      "\n",
      "Actual SUMMARY for doc  5 ::\n",
      "-----------------------------------\n",
      "Dale Smallwood, a criminal investigator for Missouri's Mark Twain National Forest looks for clues to determine the cause of forest fires. A split, charred tree stump may indicate that lightening was to blame. On the other hand, Carbon particles indicate the exhaust of a passing truck was the culprit. And there are other ways to tell a fire was accidental. However, he comes down to arson in most of his investigations. Arsonists set 172 of the 296 Mark Twain fires that have broken out so far this year. Smallwood estimates, 70 percent of the forest fires each year in Missouri are deliberately set.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Generated SUMMARY for doc  6 ::\n",
      "-----------------------------------\n",
      ";    The attitude of the public will\n",
      "determine the effectiveness of the police discipline system.\n",
      "\n",
      "\n",
      "Actual SUMMARY for doc  6 ::\n",
      "-----------------------------------\n",
      "At the end of a high-speed chase in Santa Clara County a police officer takes his baton and brutally hits the suspect on the back eight times, kicks him in the face, picks him up by the shoulders and slams his face into the pavement. The suspect suffers a dislocated shoulder, a concussion, and numerous cuts, bruises and contusions. The officer's report states that the suspect's injuries were due to a fall from his bike. None of the back-up officers reported any brutality. All too often a code of silence blocks prosecution of police brutality. That is why we need citizen review boards.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Generation Complete\n"
     ]
    }
   ],
   "source": [
    "random_summaries(0.98,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def serialize_data_matrix(poly_deg=2):\n",
    "    '''Helper function to dump the data matrix onto hard disc'''\n",
    "    poly = PolynomialFeatures(poly_deg)        \n",
    "    \n",
    "    X_Matrix = construct_X_Matrix(clust_sent_pos,clust_sent_lens,clust_mean_tfs,clust_mean_idfs,clust_mean_cfs,clust_pos_ratios,\n",
    "                                  clust_ner_ratio,clust_dig_ratio,clust_stop_word_ratio)\n",
    "    X_Matrix = poly.fit_transform(X_Matrix)\n",
    "    \n",
    "    data = np.column_stack((X_Matrix,Y))\n",
    "    serialize('data_matrix',data)\n",
    "    print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "serialize_data_matrix(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hard Baseline which assumes the very first sentence of the document as the summary</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predicted_rouge(cluster,document,clf,order,sents_in_summ=1):        \n",
    " \n",
    "    X_Matrix = construct_X_Matrix_for_test_doc(cluster,document,clust_sent_pos,clust_sent_lens,clust_mean_tfs,\n",
    "                                clust_mean_idfs,clust_mean_cfs,clust_pos_ratios,clust_ner_ratio,\n",
    "                                clust_dig_ratio,clust_stop_word_ratio,order)\n",
    "    \n",
    "    y_hats = list(clf.predict(X_Matrix))   \n",
    "    \n",
    "    pred_summary = select_sentences(cluster,document,y_hats,sents_in_summ)\n",
    "    \n",
    "    summary_path = data_root_dir+ '\\\\' + 'Summaries' + '\\\\' + document + '.txt'\n",
    "    gold_summary = extract_gold_summ_from_doc(summary_path)       \n",
    "    \n",
    "    try:\n",
    "        rogue = get_rouge_n_score(gold_summary,pred_summary,n=2,do_stem=True)                \n",
    "    except:\n",
    "        rogue = 0                \n",
    "        \n",
    "    return rogue        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_custom_model(clf,order=2,sents_in_summ=1):\n",
    "    '''Evaluate the Model'''\n",
    "    \n",
    "    rouge_lst = []\n",
    "    for clust,docs in clust_files.items():\n",
    "        for doc in docs:        \n",
    "            if doc not in docs_without_summ[clust] :\n",
    "                rouge = get_predicted_rouge(clust,doc,clf,order,sents_in_summ)                \n",
    "                rouge_lst.append(rouge)\n",
    "                \n",
    "            \n",
    "    avg = sum(rouge_lst)/len(rouge_lst)\n",
    "    return avg    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_hard_baseline_2():\n",
    "    '''Blindly assume first sentence as the predicted sentence. Compute the ROGUE_SCORE between\n",
    "       the first sentence and the actual summary.'''\n",
    "    \n",
    "    rouge_lst = []\n",
    "    for clust,docs in clust_files.items():\n",
    "        for doc in docs:        \n",
    "            if doc not in docs_without_summ[clust] :\n",
    "                first_sentence = doc_to_sent_list(doc)[0]                   \n",
    "                \n",
    "                doc_path = data_root_dir+ '\\\\' + 'Summaries' + '\\\\' + doc + '.txt'\n",
    "                summary = extract_gold_summ_from_doc(doc_path)\n",
    "\n",
    "                try:\n",
    "                    rogue = get_rouge_n_score(first_sentence,summary,n=2,do_stem=True)\n",
    "                except:                                     \n",
    "                    rogue = 0\n",
    "                \n",
    "                rouge_lst.append(rogue)\n",
    "        \n",
    "    avg = sum(rouge_lst)/len(rouge_lst)\n",
    "    return avg    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Sentence Model's accuracy 0.0817177733765\n",
      "Our Model's accuracy 0.220883478463\n"
     ]
    }
   ],
   "source": [
    "hard_baseline_accuracy = evaluate_hard_baseline_2()\n",
    "model_accuracy    = evaluate_custom_model(clf,order=2,sents_in_summ=1)\n",
    "\n",
    "print 'First Sentence Model\\'s accuracy',hard_baseline_accuracy\n",
    "print 'Our Model\\'s accuracy', model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
