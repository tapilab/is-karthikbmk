{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline using Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from pyrouge import Rouge155\n",
    "from sumy import evaluation\n",
    "from sumy.models import dom\n",
    "from sumy.nlp import tokenizers\n",
    "from stemming.porter2 import stem\n",
    "\n",
    "import os.path\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import copy\n",
    "pos_tagger = nltk.data.load(nltk.tag._POS_TAGGER)\n",
    "import pickle\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import operator\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from nltk.tag.stanford import NERTagger\n",
    "from collections import defaultdict\n",
    "import nltk.data\n",
    "java_path = \"C:/Program Files/Java/jdk1.7.0_71/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "tag_file1 = '.\\\\Others\\\\stanford_ner\\\\ner\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz'\n",
    "tag_file2 = '.\\\\Others\\\\stanford_ner\\\\ner\\\\stanford-ner.jar'\n",
    "ner_tagger = NERTagger(tag_file1,tag_file2,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Feature Extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"Others\\Features.png\" alt=\"HTML5 Icon\" width=\"800\" height=\"500\", style=\"display: ;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_root_dir = '..\\data\\DUC2001'\n",
    "annotation_file = 'annotations.txt'\n",
    "txt_opn_tag = '<TEXT>'\n",
    "txt_close_tag = '</TEXT>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_and_its_files(data_root_dir,annotation_file):\n",
    "    '''Get a Cluster and the file names associated with it\n",
    "       Returns a dictionary of the form { cluster_1 : [file1,file2,file3....], cluster_2 : [file1,file2,file3....] }'''    \n",
    "    \n",
    "    f = open(data_root_dir + '\\\\' + annotation_file,'r')\n",
    "    \n",
    "    clust_files = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    for line in f.readlines():\n",
    "        cur_line = line.split(';')[0]\n",
    "        clust_name = cur_line.split('@')[1]\n",
    "        file_name = cur_line.split('@')[0]\n",
    "        \n",
    "        clust_files[clust_name].append(file_name)\n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "    return clust_files\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AP900322-0200', 'FBIS-41815', 'FBIS-45908', 'FT921-9310', 'FT931-3883', 'FT933-8272', 'FT941-575', 'LA042290-0104', 'LA060490-0083', 'WSJ910107-0139']\n"
     ]
    }
   ],
   "source": [
    "print get_cluster_and_its_files(data_root_dir,annotation_file)['mad cow disease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_from_doc(document_path,txt_opn_tag,txt_close_tag):\n",
    "    \n",
    "    f = open(document_path,'r')\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    start = content.index(txt_opn_tag) + len(txt_opn_tag)\n",
    "    end   = content.index(txt_close_tag)\n",
    "    \n",
    "    return content[start:end]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_txt(text,nltk_flag=True,ner_flag=False):\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    if ner_flag == True:        \n",
    "        tokenizedList = re.split('[^a-zA-Z]+', text.lower())\n",
    "        return tokenizedList\n",
    "    \n",
    "    if nltk_flag == False:\n",
    "        #return [x.lower() for x in re.findall(r\"\\w+\", text)]\n",
    "\n",
    "        tokenizedList = re.split('\\W+', text.lower())\n",
    "        return [unicode(x,'utf-8') for x in tokenizedList if x != '' and x != '\\n' and x != u'\\x85' and x != '\\r' and x != '_']\n",
    "    else:\n",
    "        return nltk.word_tokenize(unicode(text,'utf-8')) \n",
    "        #return [x for x in toks if x != '' and x != '\\n' and x != u'\\x85' and x != '\\r' and x != '_' and x!= ',' and x != '.']    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'this', 'is', 'this', 'cool', 'i', 'don', 't', 'know']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_txt('What is this ?? Is this _ cool ? I don\\'t know',nltk_flag=True,ner_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 1 : Term frequency over the cluster(TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_term_freqs(data_root_dir,annotation_file,stop_words=None) :\n",
    "    '''Get the term freqs of words in clusters. The term freqs are unique to clusters.\n",
    "    Returns a dict of form {clust1 : {word1 : 2, word2 :3...},clust2 : {word1 : 2, word2 :3..} ......}'''\n",
    "        \n",
    "    #Check about stop_words\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_term_freq = defaultdict(defaultdict)\n",
    "    \n",
    "    \n",
    "    for clust,files in clust_files.iteritems():\n",
    "        term_freq = defaultdict(int)\n",
    "        \n",
    "        for doc in files:\n",
    "            doc_path = data_root_dir + '\\\\' + doc\n",
    "            txt = get_text_from_doc(doc_path,txt_opn_tag,txt_close_tag)\n",
    "            doc_tokens = tokenize_txt(txt)\n",
    "            \n",
    "            for token in doc_tokens:\n",
    "                term_freq[token] += 1\n",
    "        \n",
    "        clust_term_freq[clust] = term_freq\n",
    "    \n",
    "    return clust_term_freq\n",
    "            \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {u'all': 1, u'Union': 1, u'Kretzschmar': 1, u'Switzerland': 1, u'per': 1, u'human': 1, u'still': 1, u'decisions': 1, u'its': 1, u'European': 1, u'Jakob': 1, u'one': 1, u'March': 1, u'(': 2, u'had': 2, u',': 10, u'should': 1, u'to': 6, u'safeguards': 1, u'do': 1, u'popularly': 1, u'affected': 1, u'diseases': 1, u'than': 1, u'government': 1, u'very': 1, u'100,000': 1, u'scientists': 1, u'possible': 1, u'Gottingen': 1, u'were': 3, u'know': 1, u'not': 3, u'affect': 2, u'existing': 1, u'countries': 1, u'medicines': 1, u'50': 1, u'whether': 1, u'transmitted': 2, u'minimal': 1, u'ban': 2, u'Contaminated': 1, u'because': 1, u'humans': 4, u'bovine': 1, u'connections': 1, u'likely': 1, u'catching': 1, u'are': 1, u'encephalopathy': 1, u'further': 1, u'institutes': 1, u'agriculture': 1, u'concern': 1, u'universities': 1, u'project': 1, u'said': 3, u'imported': 3, u'for': 2, u'1992': 1, u'recorded': 1, u'expressed': 1, u'research': 4, u'I': 1, u'health': 1, u'between': 1, u'new': 1, u'contaminated': 2, u'University': 1, u'announced': 1, u'available': 1, u'be': 7, u'we': 1, u'Professor': 1, u'pushing': 1, u'EU': 2, u'by': 2, u'official': 1, u'on': 2, u'about': 1, u'last': 1, u'would': 1, u'origins': 1, u'launch': 1, u'of': 11, u'30': 1, u'discussed': 1, u'figures': 1, u'argue': 1, u'or': 2, u'comes': 1, u\"'The\": 1, u'Gerstmann': 1, u'danger': 1, u'year': 1, u'ministers': 2, u'beings': 1, u'initiative': 1, u'been': 1, u'rarely': 1, u'from': 4, u'beef': 6, u'debilitates': 1, u'union': 1, u'there': 2, u'two': 2, u'cent': 1, u'.': 11, u'2': 1, u'way': 1, u'Hans': 1, u'BSE': 5, u'meeting': 1, u'more': 1, u'-may': 1, u'spongiform': 1, u'that': 8, u'brains': 1, u'sufficient': 1, u'but': 1, u'personally': 1, u'known': 2, u'cases': 2, u'with': 1, u'eat': 1, u'2,092': 1, u'Seven': 1, u'made': 2, u'animals': 1, u'cow': 2, u'transmissible': 1, u'German': 4, u'ministry': 2, u'as': 3, u'will': 2, u'Britain': 2, u'can': 3, u'country': 2, u'Straussler': 1, u'at': 1, u'and': 6, u'non-existent': 2, u'imports': 2, u'is': 4, u'cattle': 3, u'it': 2, u'evidence': 1, u'examine': 2, u'ingredients': 1, u'have': 1, u'in': 3, u'Several': 1, u'technology': 1, u'any': 1, u'result': 1, u\"'mad\": 1, u'syndrome': 1, u'no': 1, u')': 2, u'-': 3, u'other': 2, u'Germany': 2, u'take': 1, u'which': 3, u'A': 1, u\"'s\": 2, u'arguing': 1, u'who': 2, u'yesterday': 1, u'British': 4, u'sponsored': 1, u'The': 3, u'13': 1, u'died': 1, u'conclusive': 1, u'a': 7, u\"'\": 3, u\"'However\": 1, u'Creutzfeldt': 1, u'disease': 5, u'think': 1, u'veal': 1, u'In': 1, u'tonnes': 2, u'the': 11})\n"
     ]
    }
   ],
   "source": [
    "clust_word_tfs = get_term_freqs(data_root_dir,annotation_file)\n",
    "print clust_word_tfs['cattle disease']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Feature 2 : Total document number in the datasets, divided by the frequency of documents which contains this word (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_doc_freqs(data_root_dir,annotation_file):\n",
    "    \n",
    "    '''Return a dictionary of the form {word1 : df1 , word2 : df2 ...}'''\n",
    "    '''Example : {furazabol : 154.5 , the : 1.00032}'''\n",
    "    \n",
    "    data_root_dir += '\\\\'\n",
    "    \n",
    "    docs =  [file_name for _,__,file_name in os.walk(data_root_dir)][0]\n",
    "    \n",
    "    if annotation_file in docs:\n",
    "        docs.remove(annotation_file)        \n",
    "    \n",
    "    inverted_index  = defaultdict(set)\n",
    "    \n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_path = data_root_dir + doc        \n",
    "        txt = get_text_from_doc(doc_path,txt_opn_tag,txt_close_tag)\n",
    "        doc_tokens = tokenize_txt(txt)\n",
    "        \n",
    "        for token in doc_tokens:\n",
    "            inverted_index[token].add(doc)\n",
    "    \n",
    "    \n",
    "    \n",
    "    no_of_docs = len(docs)\n",
    "    idf_dict = defaultdict(float)\n",
    "    \n",
    "    for term,doc_lst in inverted_index.iteritems():\n",
    "        idf_dict[term] = float(no_of_docs) / len(doc_lst)\n",
    "    \n",
    "    return idf_dict\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154.5\n",
      "1.00324675325\n"
     ]
    }
   ],
   "source": [
    "doc_freqs = get_doc_freqs(data_root_dir,annotation_file)\n",
    "print doc_freqs['furazabol']\n",
    "print doc_freqs['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 3 : The frequency of documents which contains this word in the current cluster (CF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clusterwise_dfs(data_root_dir,annotation_file):\n",
    "    \n",
    "    '''Return a dictionary of the form : {clust1 : (word1 : df1,word2 :df2 .....) , clust1 : (word3 : df3,word2 :df3 .....)}'''\n",
    "    '''Note that the document frequencies of term are calculated clusterwise, and not on the whole dataset'''\n",
    "    \n",
    "    clust_doc_freqs = defaultdict(defaultdict)\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():\n",
    "        inverted_index  = defaultdict(set)\n",
    "        \n",
    "        for doc in files:\n",
    "            doc_path = data_root_dir + '\\\\' + doc\n",
    "            txt = get_text_from_doc(doc_path,txt_opn_tag,txt_close_tag)\n",
    "            doc_tokens = tokenize_txt(txt)\n",
    "            \n",
    "            for token in doc_tokens:\n",
    "                inverted_index[token].add(doc)\n",
    "        \n",
    "        \n",
    "        clust_df = defaultdict(int)\n",
    "        \n",
    "        for term,doc_lst in inverted_index.iteritems():\n",
    "            clust_df[term] =  len(doc_lst)\n",
    "        \n",
    "        clust_doc_freqs[clust] = clust_df\n",
    "    \n",
    "    return clust_doc_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u',', 10), (u'encephalopathy', 10), (u'cow', 10), (u'and', 10), (u'.', 10), (u'in', 10), (u'the', 10), (u'has', 10), (u'for', 10), (u\"'s\", 10), (u'that', 10), (u'were', 10), (u'spongiform', 10), (u'of', 10), (u'with', 10), (u'as', 10), (u'to', 10), (u'a', 10), (u'be', 10), (u'from', 10)]\n"
     ]
    }
   ],
   "source": [
    "clust_dfs = get_clusterwise_dfs(data_root_dir,annotation_file)\n",
    "print sorted(clust_dfs['mad cow disease'].items(),key=operator.itemgetter(1),reverse=True)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 4 : A 4-dimension binary vector indicates whether the word is a noun, a verb, an adjective or an adverb. If the word has\n",
    "another part-of-speech, the vector is all-zero  (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_short_tag(long_tag,valid_pos=['NN','VB','JJ','RB']):      \n",
    "    '''Truncate long_tag to get its first 2 chars. If a valid POS, return first 2 chars. else return OT (Other)'''\n",
    "    '''Valid POS are NN,VB,JJ,RB'''\n",
    "    \n",
    "    valid_pos_lst = valid_pos\n",
    "       \n",
    "    long_tag = str.upper(long_tag[0:2])\n",
    "    \n",
    "    if long_tag in valid_pos_lst:\n",
    "        return long_tag\n",
    "    \n",
    "    else:\n",
    "        return 'OT'                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sentence_tags(sentence):\n",
    "    '''POS tag the words in the sentence and return a dict of the form : {word1 : [tag1,tag2..], word2 : [tag3,tag4..]..}'''\n",
    "    word_tag_dict = defaultdict(set)\n",
    "    sent_tags = pos_tagger.tag(tokenize_txt(sentence))\n",
    "        \n",
    "    for word_tag in sent_tags:\n",
    "        word = word_tag[0]\n",
    "        tag = word_tag[1]\n",
    "        \n",
    "        word_tag_dict[word].add(get_short_tag(tag))\n",
    "    \n",
    "    return word_tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'set'>, {u'sent': set(['NN']), u'one': set(['OT'])})\n",
      "defaultdict(<type 'set'>, {u'two': set(['OT']), u'sent': set(['NN'])})\n"
     ]
    }
   ],
   "source": [
    "print get_sentence_tags(\"sent one\")\n",
    "print get_sentence_tags(\"sent two\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_doc_tags(document):\n",
    "    \n",
    "    '''Perform POS tagging on all the sentences in the document and return a dict of the form :'''\n",
    "    ''' (sent_id : { word1 : tag1 ...}...}'''\n",
    "    \n",
    "    sent_and_tags = defaultdict(int)\n",
    "    \n",
    "    #sentences = document.split('.')\n",
    "    sentences = sent_detector.tokenize(document,realign_boundaries=True)\n",
    "    \n",
    "    for i,sentence in enumerate(sentences):\n",
    "        sent_and_tags[i] = get_sentence_tags(sentence.strip('.').strip('\\n'.strip('')))\n",
    "    \n",
    "    return sent_and_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: defaultdict(<type 'set'>, {u'be': set(['VB']), u'Trump': set(['NN']), u'would': set(['OT']), u'that': set(['OT']), u'apparent': set(['JJ']), u'it': set(['OT']), u'next': set(['JJ']), u'Donald': set(['NN']), u'became': set(['VB']), u'president': set(['NN']), u'the': set(['OT']), u'Today': set(['NN'])}), 1: defaultdict(<type 'set'>, {u'Clinton': set(['NN']), u'wouldnt': set(['NN']), u'However': set(['RB']), u',': set(['OT']), u'Hillary': set(['NN']), u'agree': set(['VB'])})})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_doc_tags(\"Who is Alan Turing ??. Alan was born in the United Kingdom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_tags(data_root_dir,annotation_file):\n",
    "    '''Perfom Part of Speech Tagging across all the sentences in all the documents in all the clusters'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_tags = defaultdict(defaultdict)\n",
    "    \n",
    "    i = 1\n",
    "    for clust,files in clust_files.iteritems():        \n",
    "        \n",
    "        for doc in files:\n",
    "            \n",
    "            if i %10 == 0:\n",
    "                print 'Finished tagging doc :', i\n",
    "            i += 1\n",
    "            doc_path = data_root_dir + '\\\\' + doc\n",
    "            txt = get_text_from_doc(doc_path,txt_opn_tag,txt_close_tag)\n",
    "            \n",
    "            clust_tags[clust][doc] = get_doc_tags(txt)\n",
    "            \n",
    "    return clust_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tagging doc : 10\n",
      "Finished tagging doc : 20\n",
      "Finished tagging doc : 30\n",
      "Finished tagging doc : 40\n",
      "Finished tagging doc : 50\n",
      "Finished tagging doc : 60\n",
      "Finished tagging doc : 70\n",
      "Finished tagging doc : 80\n",
      "Finished tagging doc : 90\n",
      "Finished tagging doc : 100\n",
      "Finished tagging doc : 110\n",
      "Finished tagging doc : 120\n",
      "Finished tagging doc : 130\n",
      "Finished tagging doc : 140\n",
      "Finished tagging doc : 150\n",
      "Finished tagging doc : 160\n",
      "Finished tagging doc : 170\n",
      "Finished tagging doc : 180\n",
      "Finished tagging doc : 190\n",
      "Finished tagging doc : 200\n",
      "Finished tagging doc : 210\n",
      "Finished tagging doc : 220\n",
      "Finished tagging doc : 230\n",
      "Finished tagging doc : 240\n",
      "Finished tagging doc : 250\n",
      "Finished tagging doc : 260\n",
      "Finished tagging doc : 270\n",
      "Finished tagging doc : 280\n",
      "Finished tagging doc : 290\n",
      "Finished tagging doc : 300\n"
     ]
    }
   ],
   "source": [
    "clust_tags = get_cluster_tags(data_root_dir,annotation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def serialize(file_name,data):\n",
    "    \n",
    "    with open(file_name, 'wb') as f:    \n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def deserialize(file_name):\n",
    "\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "file_name = 'pos_tags.pickle'\n",
    "#serialize(file_name,clust_tags)\n",
    "clust_tags = deserialize(file_name)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_cpy = copy.deepcopy(clust_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_pos(pos_set,pos_idx = {'NN' : 0 ,'VB' : 1,'JJ' : 2,'RB' : 3}):\n",
    "    \n",
    "    '''Convert the POS set to a binary vector according to pos_idx'''    \n",
    "    bin_pos_vec = 4*[False]\n",
    "    \n",
    "    for pos in pos_set:\n",
    "        \n",
    "        if pos == 'OT':\n",
    "            return 4*[False]\n",
    "        else:\n",
    "            bin_pos_vec[pos_idx[pos]] = True\n",
    "    \n",
    "    return bin_pos_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, False, False, True]\n",
      "[False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "print vectorize_pos({'NN','RB'})\n",
    "print vectorize_pos({'NN','RB','JJ','VB','OT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize_tags_across_clusters(clust_tags):\n",
    "    '''Binarize the POS of words'''\n",
    "\n",
    "    for clust,doc in clust_tags.iteritems(): \n",
    "\n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "\n",
    "        for doc,sent in doc.iteritems():\n",
    "\n",
    "            sent_word = defaultdict(defaultdict)\n",
    "\n",
    "            for sen_id,word_pos in sent.iteritems():\n",
    "\n",
    "\n",
    "                for word,pos in word_pos.iteritems():                            \n",
    "                    word_pos[word] = copy.deepcopy(vectorize_pos(pos))\n",
    "\n",
    "                sent_word[sen_id] = copy.deepcopy(word_pos)\n",
    "\n",
    "            doc_sent[doc] = copy.deepcopy(sent_word)\n",
    "\n",
    "        clust_tags[clust] = copy.deepcopy(doc_sent)\n",
    "\n",
    "    return clust_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_clust_tags = vectorize_tags_across_clusters(clust_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'set'>, {'and': set(['OT']), 'humans': set(['NN']), 'sales': set(['NN']), 'topic': set(['NN']), 'put': set(['VB']), 'britain': set(['NN']), 'in': set(['OT']), 'subject': set(['NN']), 'beef': set(['NN']), 'to': set(['OT']), 'crimp': set(['NN']), 'has': set(['VB']), 'be': set(['VB']), 'that': set(['OT']), 'domestic': set(['JJ']), 'pubs': set(['NN']), 'press': set(['NN']), 'a': set(['OT']), 'made': set(['VB']), 'concern': set(['NN']), 'of': set(['OT']), 'disease': set(['NN']), 'p': set(['NN']), 'transmitted': set(['VB']), 'can': set(['OT']), 'serious': set(['JJ']), 'the': set(['OT'])}) \n",
      "\n",
      "\n",
      "defaultdict(<type 'set'>, {'and': [False, False, False, False], 'humans': [True, False, False, False], 'sales': [True, False, False, False], 'topic': [True, False, False, False], 'put': [False, True, False, False], 'britain': [True, False, False, False], 'in': [False, False, False, False], 'subject': [True, False, False, False], 'beef': [True, False, False, False], 'to': [False, False, False, False], 'crimp': [True, False, False, False], 'has': [False, True, False, False], 'be': [False, True, False, False], 'that': [False, False, False, False], 'domestic': [False, False, True, False], 'pubs': [True, False, False, False], 'press': [True, False, False, False], 'a': [False, False, False, False], 'made': [False, True, False, False], 'concern': [True, False, False, False], 'of': [False, False, False, False], 'disease': [True, False, False, False], 'p': [True, False, False, False], 'transmitted': [False, True, False, False], 'can': [False, False, False, False], 'serious': [False, False, True, False], 'the': [False, False, False, False]})\n"
     ]
    }
   ],
   "source": [
    "print old_cpy['mad cow disease']['LA060490-0083'][2],'\\n\\n'\n",
    "print new_clust_tags['mad cow disease']['LA060490-0083'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 5 : A binary value equals one iff the output of the named entity classifier from CoreNLP is not empty  (Named Entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_ners(data_root_dir,annotation_file):\n",
    "    '''Perform Named Entity Recognition on all sentences in all docs in all clusters'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent,nltk_flag=True,ner_flag=True) for sent in sentences]\n",
    "            \n",
    "            sent_ner_cnt = defaultdict(int)\n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):    \n",
    "                \n",
    "                \n",
    "                ners = ner_tagger.tag(tok_sent)\n",
    "                cnt = 0\n",
    "                for ner in ners:\n",
    "                    if ner[1] != 'O':\n",
    "                        cnt += 1\n",
    "                sent_ner_cnt[s_id] = cnt\n",
    "            \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_ner_cnt)\n",
    "            \n",
    "            print 'FINISHED NER ON ', file_name\n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)\n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clust_ners = extract_ners(data_root_dir,annotation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "file_name = 'ner_tags.pickle'\n",
    "#serialize(file_name,clust_ners)\n",
    "clust_tags = deserialize(file_name)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 1, 1: 1, 2: 1, 3: 0, 4: 0, 5: 0, 6: 1, 7: 0, 8: 1, 9: 1, 10: 0, 11: 2, 12: 1, 13: 0, 14: 1, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 2, 24: 0, 25: 0, 26: 0, 27: 0, 28: 1, 29: 0})"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_ners['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 6 : A binary value denotes if a word in Number  (Number)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_digit_cnt(data_root_dir,annotation_file,cnt_ratio_flag='C'):\n",
    "    '''Count the number of digits in a sentence'''\n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent) for sent in sentences]\n",
    "            \n",
    "            sent_dig_cnt = defaultdict(int)\n",
    "            \n",
    "            dig_cnt = 0\n",
    "            for s_id,tok_sent in enumerate(sent_tokens):                                    \n",
    "                for tok in tok_sent:\n",
    "                    if tok.isdigit():\n",
    "                        dig_cnt += 1\n",
    "                if cnt_ratio_flag == 'C':\n",
    "                    sent_dig_cnt[s_id] = dig_cnt\n",
    "                else:\n",
    "                    sent_dig_cnt[s_id] = float(dig_cnt)/len(tok_sent)\n",
    "        \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_dig_cnt)            \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_digs = extract_digit_cnt(data_root_dir,annotation_file)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "clust_digs = extract_digit_cnt(data_root_dir,annotation_file)\n",
    "print 'done'\n",
    "print clust_digs['mad cow disease']['LA060490-0083'][29]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 22 : The number of digits, divided by the sentence length(Number ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.03225806451612903, 9: 0.2, 10: 0.06451612903225806, 11: 0.10526315789473684, 12: 0.08333333333333333, 13: 0.07142857142857142, 14: 0.1111111111111111, 15: 0.2, 16: 0.09090909090909091, 17: 0.2222222222222222, 18: 0.07142857142857142, 19: 0.06666666666666667, 20: 0.10714285714285714, 21: 0.125, 22: 1.0, 23: 0.10714285714285714, 24: 0.1875, 25: 0.12, 26: 0.3333333333333333, 27: 0.21428571428571427, 28: 0.10714285714285714, 29: 1.0})"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_dig_ratio = extract_digit_cnt(data_root_dir,annotation_file,'R')\n",
    "print 'done'\n",
    "clust_dig_ratio['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 23 : The number of stop words, divided by the sentence length(Stop word ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stop_word_ratio(data_root_dir,annotation_file):\n",
    "    '''Compute the stop word ratio for all sentences'''\n",
    "    '''stop word ratio == no of stop words in sent / len(sent) '''\n",
    "    \n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent) for sent in sentences]\n",
    "            \n",
    "            sent_dig_cnt = defaultdict(int)\n",
    "            \n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):      \n",
    "                stop_cnt = 0\n",
    "                for tok in tok_sent:\n",
    "                    if tok.lower() in english_stopwords:\n",
    "                        stop_cnt += 1\n",
    "                sent_dig_cnt[s_id] = float(stop_cnt)/len(tok_sent)\n",
    "        \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_dig_cnt)            \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "0.392857142857\n"
     ]
    }
   ],
   "source": [
    "clust_digs = stop_word_ratio(data_root_dir,annotation_file)\n",
    "print 'done'\n",
    "print clust_digs['mad cow disease']['LA060490-0083'][18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 24 : No of words in the sentence (Sentence Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_length(data_root_dir,annotation_file):\n",
    "    '''Compute the Lenght of sentences and store them in a dictionary'''    \n",
    "        \n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent) for sent in sentences]\n",
    "            \n",
    "            sent_dig_cnt = defaultdict(int)\n",
    "            \n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):      \n",
    "                sent_dig_cnt[s_id] = len(tok_sent)\n",
    "        \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_dig_cnt)            \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "clust_sent_lens = sent_length(data_root_dir,annotation_file)\n",
    "print 'done'\n",
    "print clust_sent_lens['mad cow disease']['LA060490-0083'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "file_path = data_root_dir + '\\\\' + 'LA060490-0083'\n",
    "doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "sentences = sent_detector.tokenize(doc)\n",
    "print len(sentences[15].split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 21 : The number of named entities divided by sentence length (NER Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ner_ratio(data_root_dir,annotation_file,clust_ners,clust_sent_lens):\n",
    "    '''Compute the Ratio of NERS : Sentence lenght and store them in a dictionary'''            \n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            total_sents = len(sent_detector.tokenize(doc))\n",
    "            #sent_tokens =[tokenize_txt(sent) for sent in sentences]\n",
    "            \n",
    "            sent_ner_ratio = defaultdict(int)\n",
    "            \n",
    "            \n",
    "            for i in range(0,total_sents):\n",
    "                sent_ner_ratio[i] = float(clust_ners[clust][file_name][i])/clust_sent_lens[clust][file_name][i]\n",
    "                        \n",
    "        \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_ner_ratio)            \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "0.105263157895\n"
     ]
    }
   ],
   "source": [
    "clust_ner_ratio = ner_ratio(data_root_dir,annotation_file,clust_ners,clust_sent_lens)\n",
    "print 'done'\n",
    "print clust_ner_ratio['mad cow disease']['LA060490-0083'][11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 20 : The number of nouns,verbs,adverbs, adjectives in the sentence, divided by the length of the sentence (POS Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_ratio(data_root_dir,annotation_file,new_clust_tags,clust_sent_lens):\n",
    "    '''Compute the Ratio of nouns,verbs,adverbs and adjectives : Sentence lenght and store them in a dictionary'''\n",
    "      \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    clusters = clust_sent_lens.keys()      \n",
    "    for clust in clusters: \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        files = clust_sent_lens[clust].keys()\n",
    "                \n",
    "        for _file in files:\n",
    "            sent_ids = clust_sent_lens[clust][_file].keys()\n",
    "            \n",
    "            sent_pos_ratio = defaultdict(int)\n",
    "            \n",
    "                \n",
    "            for sent_id in sent_ids:\n",
    "                pos_cnt = 0\n",
    "                for word,tag_lst in new_clust_tags[clust][_file][sent_id].iteritems():\n",
    "                    '''\n",
    "                    if _file == 'LA060490-0083' and sent_id == 3:\n",
    "                        print tag_lst, pos_cnt*1.0/clust_sent_lens[clust][_file][sent_id]\n",
    "                        #print new_clust_tags[clust][_file][sent_id]                    \n",
    "                    '''\n",
    "                    if True in tag_lst:\n",
    "                        pos_cnt += 1                \n",
    "                sent_pos_ratio[sent_id] = float(pos_cnt)/ clust_sent_lens[clust][_file][sent_id]\n",
    "                \n",
    "            doc_sent[_file] = copy.deepcopy(sent_pos_ratio)   \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "    \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_pos_ratios = pos_ratio(data_root_dir,annotation_file,new_clust_tags,clust_sent_lens)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5263157894736842"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_pos_ratios['mad cow disease']['LA060490-0083'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 14 : The position of the sentence. Suppose there are M sentences in the document \n",
    "    , then for the ith sentence the position is computed as 1-(i-1)/(M-1)              (POSITION)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_pos(data_root_dir,annotation_file,clust_sent_lens):\n",
    "    '''Compute the position of the sentence, according to above formula'''\n",
    "      \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    clusters = clust_sent_lens.keys()      \n",
    "    for clust in clusters: \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        files = clust_sent_lens[clust].keys()\n",
    "                \n",
    "        for _file in files:\n",
    "            sent_ids = clust_sent_lens[clust][_file].keys()\n",
    "            \n",
    "            total_sents = len(clust_sent_lens[clust][_file].keys())\n",
    "            \n",
    "            #Avoid divide by 0 error\n",
    "            if total_sents == 1:\n",
    "                total_sents = 2 \n",
    "                \n",
    "            sent_positon = defaultdict(int)\n",
    "            \n",
    "                \n",
    "            for sent_id in sent_ids:\n",
    "                sent_positon[sent_id] =  1 - ( float( sent_id ) / (total_sents - 1) )\n",
    "                \n",
    "            doc_sent[_file] = copy.deepcopy(sent_positon)   \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "    \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_sent_pos = sentence_pos(data_root_dir,annotation_file,clust_sent_lens)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 1.0, 1: 0.9655172413793104, 2: 0.9310344827586207, 3: 0.896551724137931, 4: 0.8620689655172413, 5: 0.8275862068965517, 6: 0.7931034482758621, 7: 0.7586206896551724, 8: 0.7241379310344828, 9: 0.6896551724137931, 10: 0.6551724137931034, 11: 0.6206896551724138, 12: 0.5862068965517242, 13: 0.5517241379310345, 14: 0.5172413793103448, 15: 0.48275862068965514, 16: 0.4482758620689655, 17: 0.4137931034482759, 18: 0.3793103448275862, 19: 0.3448275862068966, 20: 0.31034482758620685, 21: 0.27586206896551724, 22: 0.24137931034482762, 23: 0.2068965517241379, 24: 0.1724137931034483, 25: 0.13793103448275867, 26: 0.10344827586206895, 27: 0.06896551724137934, 28: 0.03448275862068961, 29: 0.0})"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_sent_pos['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 17 : The mean TF of all words in the sentence, divided by the sentence length (Averaged TF)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def averaged_tf(data_root_dir,annotation_file,clust_word_tfs):\n",
    "    '''Get the average TF values of words in a sentence and them in a dictionary'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:                        \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent,nltk_flag=True,ner_flag=True) for sent in sentences]\n",
    "            \n",
    "            sent_mean_tf = defaultdict(int)\n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):    \n",
    "                mean_tf = 0\n",
    "                for word in tok_sent:\n",
    "                    mean_tf += clust_word_tfs[clust][word]\n",
    "                mean_tf = float(mean_tf)/len(tok_sent)\n",
    "                \n",
    "                sent_mean_tf[s_id] = mean_tf\n",
    "            \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_mean_tf)\n",
    "                        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)\n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_mean_tfs = averaged_tf(data_root_dir,annotation_file,clust_word_tfs)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 52.46875, 1: 33.78787878787879, 2: 79.97222222222223, 3: 67.77777777777777, 4: 54.36363636363637, 5: 120.4375, 6: 39.529411764705884, 7: 42.21739130434783, 8: 27.208333333333332, 9: 6.5, 10: 46.81481481481482, 11: 67.47368421052632, 12: 39.476190476190474, 13: 62.72, 14: 33.5625, 15: 50.09090909090909, 16: 59.388888888888886, 17: 60.22222222222222, 18: 74.3103448275862, 19: 26.076923076923077, 20: 62.21739130434783, 21: 59.8, 22: 0.6666666666666666, 23: 44.31818181818182, 24: 71.92857142857143, 25: 32.57142857142857, 26: 10.333333333333334, 27: 49.63636363636363, 28: 45.65384615384615, 29: 0.0})"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_mean_tfs['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 18 : The mean IDF of all words in the sentence, divided by the sentence length (Averaged IDF)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def averaged_idf(data_root_dir,annotation_file,doc_freqs):\n",
    "    '''Get the average IDF values of words in a sentence and them in a dictionary'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:                        \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent,nltk_flag=True,ner_flag=True) for sent in sentences]\n",
    "            \n",
    "            sent_mean_idf = defaultdict(int)\n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):    \n",
    "                mean_idf = 0\n",
    "                for word in tok_sent:\n",
    "                    mean_idf += doc_freqs[word]\n",
    "                mean_idf = float(mean_idf)/len(tok_sent)\n",
    "                \n",
    "                sent_mean_idf[s_id] = mean_idf\n",
    "            \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_mean_idf)\n",
    "                        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)\n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_mean_idfs = averaged_idf(data_root_dir,annotation_file,doc_freqs)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 32.9676003655945, 1: 33.900159108011366, 2: 36.70248740133245, 3: 22.232825952937723, 4: 20.81086023515057, 5: 16.980816560429865, 6: 17.692625701242967, 7: 26.943353899985787, 8: 28.281419427609936, 9: 9.258651554752458, 10: 18.860165464137836, 11: 52.80135738098646, 12: 34.1233555537647, 13: 19.791633880215812, 14: 29.56881183615931, 15: 11.795619061560934, 16: 27.671832240526257, 17: 30.09017123117265, 18: 11.606496726037758, 19: 21.9815518896864, 20: 30.171094131562324, 21: 25.695866018971874, 22: 137.33333333333334, 23: 21.31402431675951, 24: 35.044368860474044, 25: 32.764443042707775, 26: 27.279400223496722, 27: 40.18385425177738, 28: 16.169330035486237, 29: 51.5})"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_mean_idfs['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 19 : The mean CF of all words in the sentence, divided by the sentence length (Averaged CF)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def averaged_cf(data_root_dir,annotation_file,clust_dfs):\n",
    "    '''Get the average Cluster freqs values of words in a sentence and them in a dictionary'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:                        \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent,nltk_flag=True,ner_flag=True) for sent in sentences]\n",
    "            \n",
    "            sent_mean_cf = defaultdict(int)\n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):    \n",
    "                mean_cf = 0\n",
    "                for word in tok_sent:\n",
    "                    mean_cf += clust_dfs[clust][word]\n",
    "                mean_cf = float(mean_cf)/len(tok_sent)\n",
    "                \n",
    "                sent_mean_cf[s_id] = mean_cf\n",
    "            \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_mean_cf)\n",
    "                        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)\n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_mean_cfs = averaged_cf(data_root_dir,annotation_file,clust_dfs)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 4.9375, 1: 4.121212121212121, 2: 5.694444444444445, 3: 5.0, 4: 3.9545454545454546, 5: 6.125, 6: 3.0588235294117645, 7: 3.4782608695652173, 8: 3.6666666666666665, 9: 2.625, 10: 4.333333333333333, 11: 4.684210526315789, 12: 3.1904761904761907, 13: 5.56, 14: 4.3125, 15: 3.727272727272727, 16: 5.222222222222222, 17: 5.111111111111111, 18: 6.586206896551724, 19: 4.5, 20: 5.260869565217392, 21: 5.0, 22: 0.6666666666666666, 23: 4.2272727272727275, 24: 4.214285714285714, 25: 3.9523809523809526, 26: 3.6666666666666665, 27: 3.3636363636363638, 28: 4.576923076923077, 29: 0.0})"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_mean_cfs['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rouge_n_score(sent_1,sent_2,n=2,do_stem=True):\n",
    "    '''Normalize the overlapping n-grams and return the score'''\n",
    "    '''Sentences are converted to lower-case and words are stemmed'''\n",
    "    \n",
    "    #lower\n",
    "    sent_1 = sent_1.lower()\n",
    "    sent_2 = sent_2.lower()\n",
    "    \n",
    "    \n",
    "    tokenizer = tokenizers.Tokenizer('ENGLISH')\n",
    "\n",
    "    sent_1_toks = tokenizer.to_words(sent_1)\n",
    "    sent_2_toks = tokenizer.to_words(sent_2)\n",
    "    \n",
    "    \n",
    "    #stem the sentence\n",
    "    if do_stem == True:\n",
    "        sent_1 = ' '.join([stem(tok) for tok in sent_1_toks])\n",
    "        sent_2 = ' '.join([stem(tok) for tok in sent_2_toks])\n",
    "    \n",
    "    sent_obj_1= dom.Sentence(sent_1,tokenizer)\n",
    "    sent_obj_2= dom.Sentence(sent_2,tokenizer)\n",
    "    \n",
    "    \n",
    "    return evaluation.rouge_n([sent_obj_1],[sent_obj_2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROGUE With stemming:  1.0\n",
      "ROUGE Without stemming:  0.0\n"
     ]
    }
   ],
   "source": [
    "print ('ROGUE With stemming: ' , get_rouge_n_score('ThIS HOLDING','THIS hOLD',2,True))\n",
    "print ('ROUGE Without stemming: ' , get_rouge_n_score('THIS HOLDING','THIS hOLD',2,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
