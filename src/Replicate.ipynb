{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline using Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import scale\n",
    "from sumy import evaluation\n",
    "from sumy.models import dom\n",
    "from sumy.nlp import tokenizers\n",
    "from stemming.porter2 import stem\n",
    "from os import listdir\n",
    "import os.path\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import copy\n",
    "import pickle\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import nltk.data\n",
    "\n",
    "java_path = \"C:/Program Files/Java/jdk1.7.0_71/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Feature Extraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\".\\Others\\Features.png\" alt=\"HTML5 Icon\" width=\"800\" height=\"500\", style=\"display: ;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_root_dir = '..\\data\\DUC2001'\n",
    "annotation_file = 'annotations.txt'\n",
    "txt_opn_tag = '<TEXT>'\n",
    "txt_close_tag = '</TEXT>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_and_its_files(data_root_dir,annotation_file):\n",
    "    '''Get a Cluster and the file names associated with it\n",
    "       Returns a dictionary of the form { cluster_1 : [file1,file2,file3....], cluster_2 : [file1,file2,file3....] }'''    \n",
    "    \n",
    "    f = open(data_root_dir + '\\\\' + annotation_file,'r')\n",
    "    \n",
    "    clust_files = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    for line in f.readlines():\n",
    "        cur_line = line.split(';')[0]\n",
    "        clust_name = cur_line.split('@')[1]\n",
    "        file_name = cur_line.split('@')[0]\n",
    "        \n",
    "        clust_files[clust_name].append(file_name)\n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "    return clust_files\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AP900322-0200', 'FBIS-41815', 'FBIS-45908', 'FT921-9310', 'FT931-3883', 'FT933-8272', 'FT941-575', 'LA042290-0104', 'LA060490-0083', 'WSJ910107-0139']\n"
     ]
    }
   ],
   "source": [
    "clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "print clust_files['mad cow disease']\n",
    "clust_list = clust_files.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_from_doc(document_path,txt_opn_tag,txt_close_tag):\n",
    "    \n",
    "    f = open(document_path,'r')\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    start = content.index(txt_opn_tag) + len(txt_opn_tag)\n",
    "    end   = content.index(txt_close_tag)\n",
    "    \n",
    "    return content[start:end]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_txt(text,nltk_flag=True,ner_flag=False):\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    if ner_flag == True:        \n",
    "        tokenizedList = re.split('[^a-zA-Z]+', text.lower())\n",
    "        return tokenizedList\n",
    "    \n",
    "    if nltk_flag == False:\n",
    "        #return [x.lower() for x in re.findall(r\"\\w+\", text)]\n",
    "\n",
    "        tokenizedList = re.split('\\W+', text.lower())\n",
    "        return [unicode(x,'utf-8') for x in tokenizedList if x != '' and x != '\\n' and x != u'\\x85' and x != '\\r' and x != '_']\n",
    "    else:\n",
    "        return nltk.word_tokenize(unicode(text,'utf-8')) \n",
    "        #return [x for x in toks if x != '' and x != '\\n' and x != u'\\x85' and x != '\\r' and x != '_' and x!= ',' and x != '.']    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'this', 'is', 'this', 'cool', 'i', 'don', 't', 'know']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_txt('What is this ?? Is this _ cool ? I don\\'t know',nltk_flag=True,ner_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 1 : Term frequency over the cluster(TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_term_freqs(data_root_dir,annotation_file,stop_words=None) :\n",
    "    '''Get the term freqs of words in clusters. The term freqs are unique to clusters.\n",
    "    Returns a dict of form {clust1 : {word1 : 2, word2 :3...},clust2 : {word1 : 2, word2 :3..} ......}'''\n",
    "        \n",
    "    #Check about stop_words\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_term_freq = defaultdict(defaultdict)\n",
    "    \n",
    "    \n",
    "    for clust,files in clust_files.iteritems():\n",
    "        term_freq = defaultdict(int)\n",
    "        \n",
    "        for doc in files:\n",
    "            doc_path = data_root_dir + '\\\\' + doc\n",
    "            txt = get_text_from_doc(doc_path,txt_opn_tag,txt_close_tag)\n",
    "            doc_tokens = tokenize_txt(txt)\n",
    "            \n",
    "            for token in doc_tokens:\n",
    "                term_freq[token] += 1\n",
    "        \n",
    "        clust_term_freq[clust] = term_freq\n",
    "    \n",
    "    return clust_term_freq\n",
    "            \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {u'all': 1, u'Union': 1, u'Kretzschmar': 1, u'Switzerland': 1, u'per': 1, u'human': 1, u'still': 1, u'decisions': 1, u'its': 1, u'European': 1, u'Jakob': 1, u'one': 1, u'March': 1, u'(': 2, u'had': 2, u',': 10, u'should': 1, u'to': 6, u'safeguards': 1, u'do': 1, u'popularly': 1, u'affected': 1, u'diseases': 1, u'than': 1, u'government': 1, u'very': 1, u'100,000': 1, u'scientists': 1, u'possible': 1, u'Gottingen': 1, u'were': 3, u'know': 1, u'not': 3, u'affect': 2, u'existing': 1, u'countries': 1, u'medicines': 1, u'50': 1, u'whether': 1, u'transmitted': 2, u'minimal': 1, u'ban': 2, u'Contaminated': 1, u'because': 1, u'humans': 4, u'bovine': 1, u'connections': 1, u'likely': 1, u'catching': 1, u'are': 1, u'encephalopathy': 1, u'further': 1, u'institutes': 1, u'agriculture': 1, u'concern': 1, u'universities': 1, u'project': 1, u'said': 3, u'imported': 3, u'for': 2, u'1992': 1, u'recorded': 1, u'expressed': 1, u'research': 4, u'I': 1, u'health': 1, u'between': 1, u'new': 1, u'contaminated': 2, u'University': 1, u'announced': 1, u'available': 1, u'be': 7, u'we': 1, u'Professor': 1, u'pushing': 1, u'EU': 2, u'by': 2, u'official': 1, u'on': 2, u'about': 1, u'last': 1, u'would': 1, u'origins': 1, u'launch': 1, u'of': 11, u'30': 1, u'discussed': 1, u'figures': 1, u'argue': 1, u'or': 2, u'comes': 1, u\"'The\": 1, u'Gerstmann': 1, u'danger': 1, u'year': 1, u'ministers': 2, u'beings': 1, u'initiative': 1, u'been': 1, u'rarely': 1, u'from': 4, u'beef': 6, u'debilitates': 1, u'union': 1, u'there': 2, u'two': 2, u'cent': 1, u'.': 11, u'2': 1, u'way': 1, u'Hans': 1, u'BSE': 5, u'meeting': 1, u'more': 1, u'-may': 1, u'spongiform': 1, u'that': 8, u'brains': 1, u'sufficient': 1, u'but': 1, u'personally': 1, u'known': 2, u'cases': 2, u'with': 1, u'eat': 1, u'2,092': 1, u'Seven': 1, u'made': 2, u'animals': 1, u'cow': 2, u'transmissible': 1, u'German': 4, u'ministry': 2, u'as': 3, u'will': 2, u'Britain': 2, u'can': 3, u'country': 2, u'Straussler': 1, u'at': 1, u'and': 6, u'non-existent': 2, u'imports': 2, u'is': 4, u'cattle': 3, u'it': 2, u'evidence': 1, u'examine': 2, u'ingredients': 1, u'have': 1, u'in': 3, u'Several': 1, u'technology': 1, u'any': 1, u'result': 1, u\"'mad\": 1, u'syndrome': 1, u'no': 1, u')': 2, u'-': 3, u'other': 2, u'Germany': 2, u'take': 1, u'which': 3, u'A': 1, u\"'s\": 2, u'arguing': 1, u'who': 2, u'yesterday': 1, u'British': 4, u'sponsored': 1, u'The': 3, u'13': 1, u'died': 1, u'conclusive': 1, u'a': 7, u\"'\": 3, u\"'However\": 1, u'Creutzfeldt': 1, u'disease': 5, u'think': 1, u'veal': 1, u'In': 1, u'tonnes': 2, u'the': 11})\n"
     ]
    }
   ],
   "source": [
    "clust_word_tfs = get_term_freqs(data_root_dir,annotation_file)\n",
    "print clust_word_tfs['cattle disease']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Feature 2 : Total document number in the datasets, divided by the frequency of documents which contains this word (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_doc_freqs(data_root_dir,annotation_file):\n",
    "    \n",
    "    '''Return a dictionary of the form {word1 : df1 , word2 : df2 ...}'''\n",
    "    '''Example : {furazabol : 154.5 , the : 1.00032}'''\n",
    "    \n",
    "    data_root_dir += '\\\\'\n",
    "    \n",
    "    docs =  [file_name for _,__,file_name in os.walk(data_root_dir)][0]\n",
    "    \n",
    "    if annotation_file in docs:\n",
    "        docs.remove(annotation_file)        \n",
    "    \n",
    "    inverted_index  = defaultdict(set)\n",
    "    \n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_path = data_root_dir + doc        \n",
    "        txt = get_text_from_doc(doc_path,txt_opn_tag,txt_close_tag)\n",
    "        doc_tokens = tokenize_txt(txt)\n",
    "        \n",
    "        for token in doc_tokens:\n",
    "            inverted_index[token].add(doc)\n",
    "    \n",
    "    \n",
    "    \n",
    "    no_of_docs = len(docs)\n",
    "    idf_dict = defaultdict(float)\n",
    "    \n",
    "    for term,doc_lst in inverted_index.iteritems():\n",
    "        idf_dict[term] = float(no_of_docs) / len(doc_lst)\n",
    "    \n",
    "    return idf_dict\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154.5\n",
      "1.00324675325\n"
     ]
    }
   ],
   "source": [
    "doc_freqs = get_doc_freqs(data_root_dir,annotation_file)\n",
    "print doc_freqs['furazabol']\n",
    "print doc_freqs['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 3 : The frequency of documents which contains this word in the current cluster (CF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clusterwise_dfs(data_root_dir,annotation_file):\n",
    "    \n",
    "    '''Return a dictionary of the form : {clust1 : (word1 : df1,word2 :df2 .....) , clust1 : (word3 : df3,word2 :df3 .....)}'''\n",
    "    '''Note that the document frequencies of term are calculated clusterwise, and not on the whole dataset'''\n",
    "    \n",
    "    clust_doc_freqs = defaultdict(defaultdict)\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():\n",
    "        inverted_index  = defaultdict(set)\n",
    "        \n",
    "        for doc in files:\n",
    "            doc_path = data_root_dir + '\\\\' + doc\n",
    "            txt = get_text_from_doc(doc_path,txt_opn_tag,txt_close_tag)\n",
    "            doc_tokens = tokenize_txt(txt)\n",
    "            \n",
    "            for token in doc_tokens:\n",
    "                inverted_index[token].add(doc)\n",
    "        \n",
    "        \n",
    "        clust_df = defaultdict(int)\n",
    "        \n",
    "        for term,doc_lst in inverted_index.iteritems():\n",
    "            clust_df[term] =  len(doc_lst)\n",
    "        \n",
    "        clust_doc_freqs[clust] = clust_df\n",
    "    \n",
    "    return clust_doc_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u',', 10), (u'encephalopathy', 10), (u'cow', 10), (u'and', 10), (u'.', 10), (u'in', 10), (u'the', 10), (u'has', 10), (u'for', 10), (u\"'s\", 10), (u'that', 10), (u'were', 10), (u'spongiform', 10), (u'of', 10), (u'with', 10), (u'as', 10), (u'to', 10), (u'a', 10), (u'be', 10), (u'from', 10)]\n"
     ]
    }
   ],
   "source": [
    "clust_dfs = get_clusterwise_dfs(data_root_dir,annotation_file)\n",
    "print sorted(clust_dfs['mad cow disease'].items(),key=operator.itemgetter(1),reverse=True)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 4 : A 4-dimension binary vector indicates whether the word is a noun, a verb, an adjective or an adverb. If the word has\n",
    "another part-of-speech, the vector is all-zero  (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_short_tag(long_tag,valid_pos=['NN','VB','JJ','RB']):      \n",
    "    '''Truncate long_tag to get its first 2 chars. If a valid POS, return first 2 chars. else return OT (Other)'''\n",
    "    '''Valid POS are NN,VB,JJ,RB'''\n",
    "    \n",
    "    valid_pos_lst = valid_pos\n",
    "       \n",
    "    long_tag = str.upper(long_tag[0:2])\n",
    "    \n",
    "    if long_tag in valid_pos_lst:\n",
    "        return long_tag\n",
    "    \n",
    "    else:\n",
    "        return 'OT'                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_sentence_tags(sentence):\n",
    "    '''POS tag the words in the sentence and return a dict of the form : {word1 : [tag1,tag2..], word2 : [tag3,tag4..]..}'''\n",
    "    word_tag_dict = defaultdict(set)\n",
    "    #sent_tags = pos_tagger.tag(tokenize_txt(sentence))\n",
    "    sent_tags = nltk.pos_tag(tokenize_txt(sentence))\n",
    "        \n",
    "    for word_tag in sent_tags:\n",
    "        word = word_tag[0]\n",
    "        tag = word_tag[1]\n",
    "        \n",
    "        word_tag_dict[word].add(get_short_tag(tag))\n",
    "    \n",
    "    return word_tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'set'>, {u'sent': set(['VB']), u'one': set(['OT'])})\n",
      "defaultdict(<type 'set'>, {u'two': set(['OT']), u'sent': set(['NN'])})\n"
     ]
    }
   ],
   "source": [
    "print get_sentence_tags(\"sent one\")\n",
    "print get_sentence_tags(\"sent two\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_doc_tags(document):\n",
    "    \n",
    "    '''Perform POS tagging on all the sentences in the document and return a dict of the form :'''\n",
    "    ''' (sent_id : { word1 : tag1 ...}...}'''\n",
    "    \n",
    "    sent_and_tags = defaultdict(int)\n",
    "    \n",
    "    #sentences = document.split('.')\n",
    "    sentences = sent_detector.tokenize(document,realign_boundaries=True)\n",
    "    \n",
    "    for i,sentence in enumerate(sentences):\n",
    "        sent_and_tags[i] = get_sentence_tags(sentence.strip('.').strip('\\n'.strip('')))\n",
    "    \n",
    "    return sent_and_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: defaultdict(<type 'set'>, {u'Turing': set(['NN']), u'is': set(['VB']), u'Who': set(['OT']), u'?': set(['OT']), u'Alan': set(['NN'])}), 1: defaultdict(<type 'set'>, {u'Kingdom': set(['NN']), u'United': set(['NN']), u'Alan': set(['NN']), u'born': set(['VB']), u'in': set(['OT']), u'the': set(['OT']), u'was': set(['VB'])})})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_doc_tags(\"Who is Alan Turing ??. Alan was born in the United Kingdom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_tags(data_root_dir,annotation_file):\n",
    "    '''Perfom Part of Speech Tagging across all the sentences in all the documents in all the clusters'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_tags = defaultdict(defaultdict)\n",
    "    \n",
    "    i = 1\n",
    "    for clust,files in clust_files.iteritems():        \n",
    "        \n",
    "        for doc in files:\n",
    "            \n",
    "            if i %10 == 0:\n",
    "                print 'Finished tagging doc :', i\n",
    "            i += 1\n",
    "            doc_path = data_root_dir + '\\\\' + doc\n",
    "            txt = get_text_from_doc(doc_path,txt_opn_tag,txt_close_tag)\n",
    "            \n",
    "            clust_tags[clust][doc] = get_doc_tags(txt)\n",
    "            \n",
    "    return clust_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clust_tags = get_cluster_tags(data_root_dir,annotation_file)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def serialize(file_name,data):\n",
    "    \n",
    "    with open(file_name, 'wb') as f:    \n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def deserialize(file_name):\n",
    "\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "file_name = 'pos_tags.pickle'\n",
    "#serialize(file_name,clust_tags)\n",
    "clust_tags = deserialize(file_name)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_cpy = copy.deepcopy(clust_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_pos(pos_set,pos_idx = {'NN' : 0 ,'VB' : 1,'JJ' : 2,'RB' : 3}):\n",
    "    \n",
    "    '''Convert the POS set to a binary vector according to pos_idx'''    \n",
    "    bin_pos_vec = 4*[False]\n",
    "    \n",
    "    for pos in pos_set:\n",
    "        \n",
    "        if pos == 'OT':\n",
    "            return 4*[False]\n",
    "        else:\n",
    "            bin_pos_vec[pos_idx[pos]] = True\n",
    "    \n",
    "    return bin_pos_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, False, False, True]\n",
      "[False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "print vectorize_pos({'NN','RB'})\n",
    "print vectorize_pos({'NN','RB','JJ','VB','OT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize_tags_across_clusters(clust_tags):\n",
    "    '''Binarize the POS of words'''\n",
    "\n",
    "    for clust,doc in clust_tags.iteritems(): \n",
    "\n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "\n",
    "        for doc,sent in doc.iteritems():\n",
    "\n",
    "            sent_word = defaultdict(defaultdict)\n",
    "\n",
    "            for sen_id,word_pos in sent.iteritems():\n",
    "\n",
    "\n",
    "                for word,pos in word_pos.iteritems():                            \n",
    "                    word_pos[word] = copy.deepcopy(vectorize_pos(pos))\n",
    "\n",
    "                sent_word[sen_id] = copy.deepcopy(word_pos)\n",
    "\n",
    "            doc_sent[doc] = copy.deepcopy(sent_word)\n",
    "\n",
    "        clust_tags[clust] = copy.deepcopy(doc_sent)\n",
    "\n",
    "    return clust_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_clust_tags = vectorize_tags_across_clusters(clust_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'set'>, {'and': set(['OT']), 'humans': set(['NN']), 'sales': set(['NN']), 'topic': set(['NN']), 'put': set(['VB']), 'britain': set(['NN']), 'in': set(['OT']), 'subject': set(['NN']), 'beef': set(['NN']), 'to': set(['OT']), 'crimp': set(['NN']), 'has': set(['VB']), 'be': set(['VB']), 'that': set(['OT']), 'domestic': set(['JJ']), 'pubs': set(['NN']), 'press': set(['NN']), 'a': set(['OT']), 'made': set(['VB']), 'concern': set(['NN']), 'of': set(['OT']), 'disease': set(['NN']), 'p': set(['NN']), 'transmitted': set(['VB']), 'can': set(['OT']), 'serious': set(['JJ']), 'the': set(['OT'])}) \n",
      "\n",
      "\n",
      "defaultdict(<type 'set'>, {'and': [False, False, False, False], 'humans': [True, False, False, False], 'sales': [True, False, False, False], 'topic': [True, False, False, False], 'put': [False, True, False, False], 'britain': [True, False, False, False], 'in': [False, False, False, False], 'subject': [True, False, False, False], 'beef': [True, False, False, False], 'to': [False, False, False, False], 'crimp': [True, False, False, False], 'has': [False, True, False, False], 'be': [False, True, False, False], 'that': [False, False, False, False], 'domestic': [False, False, True, False], 'pubs': [True, False, False, False], 'press': [True, False, False, False], 'a': [False, False, False, False], 'made': [False, True, False, False], 'concern': [True, False, False, False], 'of': [False, False, False, False], 'disease': [True, False, False, False], 'p': [True, False, False, False], 'transmitted': [False, True, False, False], 'can': [False, False, False, False], 'serious': [False, False, True, False], 'the': [False, False, False, False]})\n"
     ]
    }
   ],
   "source": [
    "print old_cpy['mad cow disease']['LA060490-0083'][2],'\\n\\n'\n",
    "print new_clust_tags['mad cow disease']['LA060490-0083'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 5 : A binary value equals one iff the output of the named entity classifier from CoreNLP is not empty  (Named Entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_ners(data_root_dir,annotation_file):\n",
    "    '''Perform Named Entity Recognition on all sentences in all docs in all clusters'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent,nltk_flag=True,ner_flag=True) for sent in sentences]\n",
    "            \n",
    "            sent_ner_cnt = defaultdict(int)\n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):    \n",
    "                \n",
    "                \n",
    "                ners = ner_tagger.tag(tok_sent)\n",
    "                cnt = 0\n",
    "                for ner in ners:\n",
    "                    if ner[1] != 'O':\n",
    "                        cnt += 1\n",
    "                sent_ner_cnt[s_id] = cnt\n",
    "            \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_ner_cnt)\n",
    "            \n",
    "            print 'FINISHED NER ON ', file_name\n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)\n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "clust_ners = extract_ners(data_root_dir,annotation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "file_name = 'ner_tags.pickle'\n",
    "#serialize(file_name,clust_ners)\n",
    "clust_ners = deserialize(file_name)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 1, 1: 1, 2: 1, 3: 0, 4: 0, 5: 0, 6: 1, 7: 0, 8: 1, 9: 1, 10: 0, 11: 2, 12: 1, 13: 0, 14: 1, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 2, 24: 0, 25: 0, 26: 0, 27: 0, 28: 1, 29: 0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_ners['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 6 : A binary value denotes if a word in Number  (Number)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_digit_cnt(data_root_dir,annotation_file,cnt_ratio_flag='C'):\n",
    "    '''Count the number of digits in a sentence'''\n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent) for sent in sentences]\n",
    "            \n",
    "            sent_dig_cnt = defaultdict(int)\n",
    "            \n",
    "            dig_cnt = 0\n",
    "            for s_id,tok_sent in enumerate(sent_tokens):                                    \n",
    "                for tok in tok_sent:\n",
    "                    if tok.isdigit():\n",
    "                        dig_cnt += 1\n",
    "                if cnt_ratio_flag == 'C':\n",
    "                    sent_dig_cnt[s_id] = dig_cnt\n",
    "                else:\n",
    "                    sent_dig_cnt[s_id] = float(dig_cnt)/len(tok_sent)\n",
    "        \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_dig_cnt)            \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_digs = extract_digit_cnt(data_root_dir,annotation_file)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print clust_digs['mad cow disease']['LA060490-0083'][29]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 22 : The number of digits, divided by the sentence length(Number ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.03225806451612903, 9: 0.2, 10: 0.06451612903225806, 11: 0.10526315789473684, 12: 0.08333333333333333, 13: 0.07142857142857142, 14: 0.1111111111111111, 15: 0.2, 16: 0.09090909090909091, 17: 0.2222222222222222, 18: 0.07142857142857142, 19: 0.06666666666666667, 20: 0.10714285714285714, 21: 0.125, 22: 1.0, 23: 0.10714285714285714, 24: 0.1875, 25: 0.12, 26: 0.3333333333333333, 27: 0.21428571428571427, 28: 0.10714285714285714, 29: 1.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_dig_ratio = extract_digit_cnt(data_root_dir,annotation_file,'R')\n",
    "print 'done'\n",
    "clust_dig_ratio['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 23 : The number of stop words, divided by the sentence length(Stop word ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stop_word_ratio(data_root_dir,annotation_file):\n",
    "    '''Compute the stop word ratio for all sentences'''\n",
    "    '''stop word ratio == no of stop words in sent / len(sent) '''\n",
    "    \n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent) for sent in sentences]\n",
    "            \n",
    "            sent_dig_cnt = defaultdict(int)\n",
    "            \n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):      \n",
    "                stop_cnt = 0\n",
    "                for tok in tok_sent:\n",
    "                    if tok.lower() in english_stopwords:\n",
    "                        stop_cnt += 1\n",
    "                sent_dig_cnt[s_id] = float(stop_cnt)/len(tok_sent)\n",
    "        \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_dig_cnt)            \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "0.392857142857\n"
     ]
    }
   ],
   "source": [
    "clust_stop_word_ratio = stop_word_ratio(data_root_dir,annotation_file)\n",
    "print 'done'\n",
    "print clust_stop_word_ratio['mad cow disease']['LA060490-0083'][18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 24 : No of words in the sentence (Sentence Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_length(data_root_dir,annotation_file):\n",
    "    '''Compute the Lenght of sentences and store them in a dictionary'''    \n",
    "        \n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent) for sent in sentences]\n",
    "            \n",
    "            sent_dig_cnt = defaultdict(int)\n",
    "            \n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):      \n",
    "                sent_dig_cnt[s_id] = len(tok_sent)\n",
    "        \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_dig_cnt)            \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "clust_sent_lens = sent_length(data_root_dir,annotation_file)\n",
    "print 'done'\n",
    "print clust_sent_lens['mad cow disease']['LA060490-0083'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "file_path = data_root_dir + '\\\\' + 'LA060490-0083'\n",
    "doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "sentences = sent_detector.tokenize(doc)\n",
    "print len(sentences[15].split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 21 : The number of named entities divided by sentence length (NER Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ner_ratio(data_root_dir,annotation_file,clust_ners,clust_sent_lens):\n",
    "    '''Compute the Ratio of NERS : Sentence lenght and store them in a dictionary'''            \n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:            \n",
    "            \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            total_sents = len(sent_detector.tokenize(doc))\n",
    "            #sent_tokens =[tokenize_txt(sent) for sent in sentences]\n",
    "            \n",
    "            sent_ner_ratio = defaultdict(int)\n",
    "            \n",
    "            \n",
    "            for i in range(0,total_sents):\n",
    "                sent_ner_ratio[i] = float(clust_ners[clust][file_name][i])/clust_sent_lens[clust][file_name][i]\n",
    "                        \n",
    "        \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_ner_ratio)            \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "0.105263157895\n"
     ]
    }
   ],
   "source": [
    "clust_ner_ratio = ner_ratio(data_root_dir,annotation_file,clust_ners,clust_sent_lens)\n",
    "print 'done'\n",
    "print clust_ner_ratio['mad cow disease']['LA060490-0083'][11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 20 : The number of nouns,verbs,adverbs, adjectives in the sentence, divided by the length of the sentence (POS Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_ratio(data_root_dir,annotation_file,new_clust_tags,clust_sent_lens):\n",
    "    '''Compute the Ratio of nouns,verbs,adverbs and adjectives : Sentence lenght and store them in a dictionary'''\n",
    "      \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    clusters = clust_sent_lens.keys()      \n",
    "    for clust in clusters: \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        files = clust_sent_lens[clust].keys()\n",
    "                \n",
    "        for _file in files:\n",
    "            sent_ids = clust_sent_lens[clust][_file].keys()\n",
    "            \n",
    "            sent_pos_ratio = defaultdict(int)\n",
    "            \n",
    "                \n",
    "            for sent_id in sent_ids:\n",
    "                pos_cnt = 0\n",
    "                for word,tag_lst in new_clust_tags[clust][_file][sent_id].iteritems():\n",
    "                    '''\n",
    "                    if _file == 'LA060490-0083' and sent_id == 3:\n",
    "                        print tag_lst, pos_cnt*1.0/clust_sent_lens[clust][_file][sent_id]\n",
    "                        #print new_clust_tags[clust][_file][sent_id]                    \n",
    "                    '''\n",
    "                    if True in tag_lst:\n",
    "                        pos_cnt += 1                \n",
    "                sent_pos_ratio[sent_id] = float(pos_cnt)/ clust_sent_lens[clust][_file][sent_id]\n",
    "                \n",
    "            doc_sent[_file] = copy.deepcopy(sent_pos_ratio)   \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "    \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_pos_ratios = pos_ratio(data_root_dir,annotation_file,new_clust_tags,clust_sent_lens)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5263157894736842"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_pos_ratios['mad cow disease']['LA060490-0083'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 14 : The position of the sentence. Suppose there are M sentences in the document \n",
    "    , then for the ith sentence the position is computed as 1-(i-1)/(M-1)              (POSITION)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_pos(data_root_dir,annotation_file,clust_sent_lens):\n",
    "    '''Compute the position of the sentence, according to above formula'''\n",
    "      \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    clusters = clust_sent_lens.keys()      \n",
    "    for clust in clusters: \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        files = clust_sent_lens[clust].keys()\n",
    "                \n",
    "        for _file in files:\n",
    "            sent_ids = clust_sent_lens[clust][_file].keys()\n",
    "            \n",
    "            total_sents = len(clust_sent_lens[clust][_file].keys())\n",
    "            \n",
    "            #Avoid divide by 0 error\n",
    "            if total_sents == 1:\n",
    "                total_sents = 2 \n",
    "                \n",
    "            sent_positon = defaultdict(int)\n",
    "            \n",
    "                \n",
    "            for sent_id in sent_ids:\n",
    "                sent_positon[sent_id] =  1 - ( float( sent_id ) / (total_sents - 1) )\n",
    "                \n",
    "            doc_sent[_file] = copy.deepcopy(sent_positon)   \n",
    "        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)        \n",
    "    \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_sent_pos = sentence_pos(data_root_dir,annotation_file,clust_sent_lens)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 1.0, 1: 0.9655172413793104, 2: 0.9310344827586207, 3: 0.896551724137931, 4: 0.8620689655172413, 5: 0.8275862068965517, 6: 0.7931034482758621, 7: 0.7586206896551724, 8: 0.7241379310344828, 9: 0.6896551724137931, 10: 0.6551724137931034, 11: 0.6206896551724138, 12: 0.5862068965517242, 13: 0.5517241379310345, 14: 0.5172413793103448, 15: 0.48275862068965514, 16: 0.4482758620689655, 17: 0.4137931034482759, 18: 0.3793103448275862, 19: 0.3448275862068966, 20: 0.31034482758620685, 21: 0.27586206896551724, 22: 0.24137931034482762, 23: 0.2068965517241379, 24: 0.1724137931034483, 25: 0.13793103448275867, 26: 0.10344827586206895, 27: 0.06896551724137934, 28: 0.03448275862068961, 29: 0.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_sent_pos['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 17 : The mean TF of all words in the sentence, divided by the sentence length (Averaged TF)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def averaged_tf(data_root_dir,annotation_file,clust_word_tfs):\n",
    "    '''Get the average TF values of words in a sentence and them in a dictionary'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:                        \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent,nltk_flag=True,ner_flag=True) for sent in sentences]\n",
    "            \n",
    "            sent_mean_tf = defaultdict(int)\n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):    \n",
    "                mean_tf = 0\n",
    "                for word in tok_sent:\n",
    "                    mean_tf += clust_word_tfs[clust][word]\n",
    "                mean_tf = float(mean_tf)/len(tok_sent)\n",
    "                \n",
    "                sent_mean_tf[s_id] = mean_tf\n",
    "            \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_mean_tf)\n",
    "                        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)\n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_mean_tfs = averaged_tf(data_root_dir,annotation_file,clust_word_tfs)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 52.46875, 1: 33.78787878787879, 2: 79.97222222222223, 3: 67.77777777777777, 4: 54.36363636363637, 5: 120.4375, 6: 39.529411764705884, 7: 42.21739130434783, 8: 27.208333333333332, 9: 6.5, 10: 46.81481481481482, 11: 67.47368421052632, 12: 39.476190476190474, 13: 62.72, 14: 33.5625, 15: 50.09090909090909, 16: 59.388888888888886, 17: 60.22222222222222, 18: 74.3103448275862, 19: 26.076923076923077, 20: 62.21739130434783, 21: 59.8, 22: 0.6666666666666666, 23: 44.31818181818182, 24: 71.92857142857143, 25: 32.57142857142857, 26: 10.333333333333334, 27: 49.63636363636363, 28: 45.65384615384615, 29: 0.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_mean_tfs['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 18 : The mean IDF of all words in the sentence, divided by the sentence length (Averaged IDF)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def averaged_idf(data_root_dir,annotation_file,doc_freqs):\n",
    "    '''Get the average IDF values of words in a sentence and them in a dictionary'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:                        \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent,nltk_flag=True,ner_flag=True) for sent in sentences]\n",
    "            \n",
    "            sent_mean_idf = defaultdict(int)\n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):    \n",
    "                mean_idf = 0\n",
    "                for word in tok_sent:\n",
    "                    mean_idf += doc_freqs[word]\n",
    "                mean_idf = float(mean_idf)/len(tok_sent)\n",
    "                \n",
    "                sent_mean_idf[s_id] = mean_idf\n",
    "            \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_mean_idf)\n",
    "                        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)\n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_mean_idfs = averaged_idf(data_root_dir,annotation_file,doc_freqs)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 32.9676003655945, 1: 33.900159108011366, 2: 36.70248740133245, 3: 22.232825952937723, 4: 20.81086023515057, 5: 16.980816560429865, 6: 17.692625701242967, 7: 26.943353899985787, 8: 28.281419427609936, 9: 9.258651554752458, 10: 18.860165464137836, 11: 52.797447968030994, 12: 34.1233555537647, 13: 19.791633880215812, 14: 29.56881183615931, 15: 11.795619061560934, 16: 27.671832240526257, 17: 30.09017123117265, 18: 11.606496726037758, 19: 21.9815518896864, 20: 30.171094131562324, 21: 25.695866018971874, 22: 137.33333333333334, 23: 21.31402431675951, 24: 35.044368860474044, 25: 32.764443042707775, 26: 27.279400223496722, 27: 40.18385425177738, 28: 16.169330035486237, 29: 51.5})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_mean_idfs['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Feature 19 : The mean CF of all words in the sentence, divided by the sentence length (Averaged CF)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def averaged_cf(data_root_dir,annotation_file,clust_dfs):\n",
    "    '''Get the average Cluster freqs values of words in a sentence and them in a dictionary'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    clust_doc = defaultdict(defaultdict)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():    \n",
    "        \n",
    "        doc_sent = defaultdict(defaultdict)\n",
    "        \n",
    "        for file_name in files:                        \n",
    "            \n",
    "            file_path = data_root_dir + '\\\\' + file_name\n",
    "            doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "            sentences = sent_detector.tokenize(doc)\n",
    "            sent_tokens =[tokenize_txt(sent,nltk_flag=True,ner_flag=True) for sent in sentences]\n",
    "            \n",
    "            sent_mean_cf = defaultdict(int)\n",
    "            \n",
    "            for s_id,tok_sent in enumerate(sent_tokens):    \n",
    "                mean_cf = 0\n",
    "                for word in tok_sent:\n",
    "                    mean_cf += clust_dfs[clust][word]\n",
    "                mean_cf = float(mean_cf)/len(tok_sent)\n",
    "                \n",
    "                sent_mean_cf[s_id] = mean_cf\n",
    "            \n",
    "            doc_sent[file_name] = copy.deepcopy(sent_mean_cf)\n",
    "                        \n",
    "        clust_doc[clust] = copy.deepcopy(doc_sent)\n",
    "        \n",
    "    return clust_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "clust_mean_cfs = averaged_cf(data_root_dir,annotation_file,clust_dfs)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {0: 4.9375, 1: 4.121212121212121, 2: 5.694444444444445, 3: 5.0, 4: 3.9545454545454546, 5: 6.125, 6: 3.0588235294117645, 7: 3.4782608695652173, 8: 3.6666666666666665, 9: 2.625, 10: 4.333333333333333, 11: 4.684210526315789, 12: 3.1904761904761907, 13: 5.56, 14: 4.3125, 15: 3.727272727272727, 16: 5.222222222222222, 17: 5.111111111111111, 18: 6.586206896551724, 19: 4.5, 20: 5.260869565217392, 21: 5.0, 22: 0.6666666666666666, 23: 4.2272727272727275, 24: 4.214285714285714, 25: 3.9523809523809526, 26: 3.6666666666666665, 27: 3.3636363636363638, 28: 4.576923076923077, 29: 0.0})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clust_mean_cfs['mad cow disease']['LA060490-0083']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_rouge_n_score(sent_1,sent_2,n=2,do_stem=True):\n",
    "    '''Normalize the overlapping n-grams and return the score'''\n",
    "    '''Sentences are converted to lower-case and words are stemmed'''\n",
    "    \n",
    "    #lower\n",
    "    sent_1 = sent_1.lower()\n",
    "    sent_2 = sent_2.lower()\n",
    "    \n",
    "    \n",
    "    tokenizer = tokenizers.Tokenizer('ENGLISH')\n",
    "\n",
    "    sent_1_toks = tokenizer.to_words(sent_1)\n",
    "    sent_2_toks = tokenizer.to_words(sent_2)\n",
    "    \n",
    "    \n",
    "    #stem the sentence\n",
    "    if do_stem == True:\n",
    "        sent_1 = ' '.join([stem(tok) for tok in sent_1_toks])\n",
    "        sent_2 = ' '.join([stem(tok) for tok in sent_2_toks])\n",
    "    \n",
    "    \n",
    "    sent_obj_1= dom.Sentence(sent_1,tokenizer)\n",
    "    sent_obj_2= dom.Sentence(sent_2,tokenizer)\n",
    "    \n",
    "    \n",
    "    return evaluation.rouge_n([sent_obj_1],[sent_obj_2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROGUE With stemming:  1.0\n",
      "ROUGE Without stemming:  0.666666666667\n"
     ]
    }
   ],
   "source": [
    "print 'ROGUE With stemming: ' , get_rouge_n_score('This iS SentENce CooLing','This is Sentence cool',2,True)\n",
    "print 'ROUGE Without stemming: ' , get_rouge_n_score('This iS SentENce CooLing','This is Sentence cool',2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_docs_without_summary(data_root_dir,annotation_file):\n",
    "    '''Return a dictionary of the form {clust1 : [doc1,doc2...],clust2 : [doc1,doc2...] ....}'''\n",
    "    '''The key is the cluster name, the value is a list of documents, for which summary do not exist'''\n",
    "    '''This is because, certain documents in the DUC dataset do not have a summary. To weed out such documents, this function\n",
    "       will be called.'''\n",
    "    \n",
    "    clust_files = get_cluster_and_its_files(data_root_dir,annotation_file)\n",
    "    \n",
    "    files_with_summ = set( [fname.lower() for fname in listdir(data_root_dir+ '\\\\' + 'Summaries' + '\\\\')] )\n",
    "    clust_docs_wo_summ = defaultdict(list)\n",
    "    \n",
    "    for clust,files in clust_files.iteritems():\n",
    "        for _file in files:\n",
    "            tmp = _file + '.txt'\n",
    "            if tmp.lower() not in files_with_summ:\n",
    "                clust_docs_wo_summ[clust].append(_file)\n",
    "    \n",
    "    return clust_docs_wo_summ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'list'>, {'ben johnson': ['AP880928-0054'], 'tornadoes': ['AP891116-0191', 'LA070190-0073'], 'crash': ['FBIS4-4674'], 'illegal aliens': ['LA071589-0076'], 'u.s. tanker spill': ['AP830325-0143'], 'north american free trade agreement': ['FT934-10911']})\n"
     ]
    }
   ],
   "source": [
    "docs_without_summ = get_docs_without_summary(data_root_dir,annotation_file)\n",
    "print docs_without_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_gold_summ_from_doc(document_path):\n",
    "    '''Extract the Gold summary of a document.'''\n",
    "    '''Gold summary is of the form <Abstract:> This is the summary <Introduction:>'''\n",
    "    start_tag = 'Abstract:'\n",
    "    close_tag = 'Introduction:'\n",
    "    \n",
    "    f = open(document_path,'r')\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    start = content.index(start_tag) + len(start_tag)\n",
    "    end   = content.index(close_tag)\n",
    "    \n",
    "    return content[start:end].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lawmakers are having difficulty coming to an agreement on whether or not illegal aliens should be counted in the 1990 census. The U.S. Constitution requires the Census Bureau to count all the \"persons\" in the country every ten years for purposes of reapportionment of seats in the house of representatives. It does not specify citizens. Some representatives feel that illegal aliens, if counted, may give some states like California and Texas and extra seat. Since there is a 435-seat limit, some representatives fear that their states may loose a seat. In the past, every state has counted legal and illegal aliens in the census.\n"
     ]
    }
   ],
   "source": [
    "doc_path = data_root_dir+ '\\\\' + 'Summaries' + '\\\\' + 'ap880623-0135.txt'\n",
    "summ = extract_gold_summ_from_doc(doc_path)\n",
    "print summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Construct the train Matrix with the dimensions M*N,where: \n",
    "<b>N = $\\sum_{i=1}^c\\sum_{j=1}^{d_i}{X_{ij}}$ where c is the no of clusters, ${d_i}$ is the no of docs in ${cluster_i}$ , $X_{ij}$ is the no of sentences in $j^{th}$ doc of $i^{th} cluster$<b><br>\n",
    "<b>M = no of features for every sentence</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_dict_to_feature_column(clust_files,docs_without_summ):\n",
    "    '''Convert the nested dictionary to a feature column'''    \n",
    "    feature_col = []\n",
    "    \n",
    "    clusters = sorted(clust_files.keys())\n",
    "    \n",
    "    for clust in clusters:         \n",
    "        \n",
    "        files = sorted(clust_files[clust].keys())\n",
    "                \n",
    "        for _file in files:\n",
    "            \n",
    "            #Ignore the docs that do not have a summary.            \n",
    "            if _file not in docs_without_summ[clust]:\n",
    "                sent_ids = sorted(clust_files[clust][_file].keys())                            \n",
    "                for sent_id in sent_ids:\n",
    "                    feature_col.append(clust_files[clust][_file][sent_id])            \n",
    "\n",
    "    \n",
    "    return np.array(feature_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_X_Matrix(clust_sent_pos,clust_sent_lens,clust_mean_tfs,clust_mean_idfs,clust_mean_cfs,clust_pos_ratios,\n",
    "                       clust_ner_ratio,clust_dig_ratio,clust_stop_word_ratio):\n",
    "    \n",
    "    '''Construct the X_Matrix by stacking the Features, columnwise, for all sentences. Finally return X_train'''\n",
    "    \n",
    "    F_position        = convert_dict_to_feature_column(clust_sent_pos,docs_without_summ)\n",
    "    F_length          = convert_dict_to_feature_column(clust_sent_lens,docs_without_summ)\n",
    "    F_mean_tfs        = convert_dict_to_feature_column(clust_mean_tfs,docs_without_summ)\n",
    "    F_mean_idfs       = convert_dict_to_feature_column(clust_mean_idfs,docs_without_summ)\n",
    "    F_mean_cfs        = convert_dict_to_feature_column(clust_mean_cfs,docs_without_summ)\n",
    "    F_pos_ratio       = convert_dict_to_feature_column(clust_pos_ratios,docs_without_summ)\n",
    "    F_ner_ratio       = convert_dict_to_feature_column(clust_ner_ratio,docs_without_summ)\n",
    "    F_dig_ratio       = convert_dict_to_feature_column(clust_dig_ratio,docs_without_summ)\n",
    "    F_stop_word_ratio = convert_dict_to_feature_column(clust_stop_word_ratio,docs_without_summ)\n",
    "    \n",
    "    stack = (F_position,F_length,F_mean_tfs,F_mean_idfs,F_mean_cfs,F_pos_ratio,F_ner_ratio,F_dig_ratio,F_stop_word_ratio)\n",
    "    return scale(np.column_stack(stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construct_Y(clust_files,docs_without_summ):\n",
    "    '''Construct the Y output value(ROGUE Score) for every sentence in the document, along\n",
    "       with the gold summary of the document.  i.e ROGUE(sentence,summary)'''    \n",
    "    feature_col = []\n",
    "    \n",
    "    clusters = sorted(clust_files.keys())\n",
    "    \n",
    "    for clust in clusters:         \n",
    "        \n",
    "        files = sorted(clust_files[clust])\n",
    "                \n",
    "        for _file in files:\n",
    "            \n",
    "            #Ignore the docs that do not have a summary.            \n",
    "            if _file not in docs_without_summ[clust]:\n",
    "\n",
    "                file_path = data_root_dir + '\\\\' + _file\n",
    "                doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "                sentences = sent_detector.tokenize(doc)\n",
    "                \n",
    "                sum_file_path = data_root_dir+ '\\\\' + 'Summaries' + '\\\\' + _file + '.txt'\n",
    "                gold_summ = extract_gold_summ_from_doc(sum_file_path)\n",
    "                \n",
    "                for sent in sentences:\n",
    "                    try:\n",
    "                        rouge_score = get_rouge_n_score(sent,gold_summ)                        \n",
    "                    except:\n",
    "                        rouge_score = 0\n",
    "                    \n",
    "                    #To avoid divide by zero error\n",
    "                    feature_col.append(rouge_score)\n",
    "                        \n",
    "    \n",
    "    return np.array(feature_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_cross_validation(X_Matrix,Y,clf,n_folds,degree):\n",
    "    '''Perform n-fold cross validation.    \n",
    "    Params:\n",
    "        X.........a Matrix of features\n",
    "        y.........the true rogue score of each sentence\n",
    "        n_folds...the number of folds of cross-validation to do\n",
    "        \n",
    "    Return:\n",
    "        the average testing accuracy across all folds.'''\n",
    "    \n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_Matrix = poly.fit_transform(X_Matrix)\n",
    "    \n",
    "    accuracies = []\n",
    "    cv = KFold(len(Y), n_folds)\n",
    "    for train_idx, test_idx in cv:        \n",
    "        clf.fit(X_Matrix[train_idx], Y[train_idx])\n",
    "        predicted = clf.predict(X_Matrix[test_idx])        \n",
    "        error = np.mean(np.abs(predicted - Y[test_idx]))\n",
    "        accuracies.append(error)                \n",
    "    avg = np.mean(accuracies)    \n",
    "    return avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_accuracies(X_Matrix,Y,clf,n_folds=10,poly_degrees=[1,2,3,4]):\n",
    "    '''Plot a graph of Test Error vs Polynomial Order'''        \n",
    "    errors = [do_cross_validation(X_Matrix,Y,clf,n_folds,degree) for degree in poly_degrees]            \n",
    "    plt.ylabel('Test Error')\n",
    "    plt.xlabel('Polynomial Degree')\n",
    "    plt.plot(poly_degrees, errors,'r-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_best_order(n_folds,Y,poly_degrees):    \n",
    "    '''Experiment with various settings and figure the best polynomial setting'''    \n",
    "    X_Matrix = construct_X_Matrix(clust_sent_pos,clust_sent_lens,clust_mean_tfs,clust_mean_idfs,clust_mean_cfs,clust_pos_ratios,\n",
    "                           clust_ner_ratio,clust_dig_ratio,clust_stop_word_ratio)\n",
    "    \n",
    "    \n",
    "    plot_accuracies(X_Matrix,Y,Ridge(),n_folds,poly_degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEPCAYAAACQmrmQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13e52d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y = construct_Y(clust_files,docs_without_summ)\n",
    "n_folds=10\n",
    "poly_degrees=[1,2,3]\n",
    "find_best_order(n_folds,Y,poly_degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Seems the Test error is minimum when the polynomial order is 2. Raise the X_Matrix to this order and fit the Classifer</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_clf(best_order,Y,clf):\n",
    "    \n",
    "    poly = PolynomialFeatures(best_order)        \n",
    "    \n",
    "    X_Matrix = construct_X_Matrix(clust_sent_pos,clust_sent_lens,clust_mean_tfs,clust_mean_idfs,clust_mean_cfs,clust_pos_ratios,\n",
    "                           clust_ner_ratio,clust_dig_ratio,clust_stop_word_ratio)\n",
    "    X_Matrix = poly.fit_transform(X_Matrix)\n",
    "    \n",
    "    print X_Matrix\n",
    "    clf.fit(X_Matrix,Y)\n",
    "    print '\\nFitted Regressor with best settings'\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e+00   1.00000000e+00   3.20000000e+01 ...,   0.00000000e+00\n",
      "    0.00000000e+00   1.18164062e-01]\n",
      " [  1.00000000e+00   9.78260870e-01   2.50000000e+01 ...,   0.00000000e+00\n",
      "    0.00000000e+00   1.93600000e-01]\n",
      " [  1.00000000e+00   9.56521739e-01   2.80000000e+01 ...,   1.27551020e-03\n",
      "    1.27551020e-02   1.27551020e-01]\n",
      " ..., \n",
      " [  1.00000000e+00   3.44827586e-02   2.40000000e+01 ...,   2.77777778e-02\n",
      "    7.63888889e-02   2.10069444e-01]\n",
      " [  1.00000000e+00   1.72413793e-02   3.40000000e+01 ...,   1.38408304e-02\n",
      "    2.76816609e-02   5.53633218e-02]\n",
      " [  1.00000000e+00   0.00000000e+00   1.00000000e+01 ...,   1.60000000e-01\n",
      "    1.20000000e-01   9.00000000e-02]]\n",
      "\n",
      "Fitted Regressor with best settings\n"
     ]
    }
   ],
   "source": [
    "clf = get_best_clf(2,Y,Ridge())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_X_Matrix_for_test_doc(cluster,document,clust_sent_pos,clust_sent_lens,clust_mean_tfs,clust_mean_idfs,\n",
    "                                    clust_mean_cfs,clust_pos_ratios,clust_ner_ratio,clust_dig_ratio,\n",
    "                                    clust_stop_word_ratio,poly_order):\n",
    "    \n",
    "    '''Extract all the features for a given document and return the extracted features'''\n",
    "    \n",
    "    X_Matrix = []\n",
    "    \n",
    "    for sent_id in clust_sent_pos[cluster][document].keys():                \n",
    "        \n",
    "        F_position = clust_sent_pos[cluster][document][sent_id]\n",
    "        F_length = clust_sent_lens[cluster][document][sent_id]\n",
    "        F_mean_tfs = clust_mean_tfs[cluster][document][sent_id]\n",
    "        F_mean_idfs = clust_mean_idfs[cluster][document][sent_id]\n",
    "        F_mean_cfs = clust_sent_pos[cluster][document][sent_id]\n",
    "        F_pos_ratio = clust_pos_ratios[cluster][document][sent_id]\n",
    "        F_ner_ratio = clust_ner_ratio[cluster][document][sent_id]\n",
    "        F_dig_ratio = clust_dig_ratio[cluster][document][sent_id]\n",
    "        F_stop_word_ratio = clust_stop_word_ratio[cluster][document][sent_id]\n",
    "        \n",
    "        row = [F_position,F_length,F_mean_tfs,F_mean_idfs,F_mean_cfs,\n",
    "                        F_pos_ratio,F_ner_ratio,F_dig_ratio,F_stop_word_ratio]\n",
    "        \n",
    "        X_Matrix.append(row)\n",
    "    \n",
    "    poly = PolynomialFeatures(poly_order)\n",
    "    X_Matrix = poly.fit_transform(np.array(X_Matrix))\n",
    "    \n",
    "    return X_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_summary(cluster,document,clf,order,i,sents_in_summ=2):\n",
    "    \n",
    "    '''Generate the summary for a document with sents_in_summ number of sentences in it'''\n",
    "    \n",
    "    X_Matrix = construct_X_Matrix_for_test_doc(cluster,document,clust_sent_pos,clust_sent_lens,clust_mean_tfs,\n",
    "                                clust_mean_idfs,clust_mean_cfs,clust_pos_ratios,clust_ner_ratio,\n",
    "                                clust_dig_ratio,clust_stop_word_ratio,order)\n",
    "    \n",
    "    y_hats = list(clf.predict(X_Matrix))    \n",
    "    top_scores = sorted(y_hats,reverse=True)[0:sents_in_summ]    \n",
    "    top_sentences_idxs = [y_hats.index(score) for score in top_scores]\n",
    "    \n",
    "    file_path = data_root_dir + '\\\\' + document\n",
    "    doc = get_text_from_doc(file_path,txt_opn_tag,txt_close_tag)\n",
    "    sentences = sent_detector.tokenize(doc)\n",
    "    \n",
    "    \n",
    "    print '\\nGENERATED SUMMARY for doc ',i, '::\\n-----------------------------------'    \n",
    "    summary = ''.join([sentences[idx] for idx in sorted(top_sentences_idxs)])\n",
    "    print str.replace(str.replace(summary,'<P>',''),'</P>','').strip()\n",
    "    print '\\n'        \n",
    "    \n",
    "    print 'ACTUAL SUMMARY for doc ',i, '::\\n-----------------------------------'    \n",
    "    summary_path = data_root_dir+ '\\\\' + 'Summaries' + '\\\\' + document + '.txt'\n",
    "    print extract_gold_summ_from_doc(summary_path)\n",
    "    print '\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~'\n",
    "    \n",
    "    '''\n",
    "    print 'COMPLETE TEXT :: \\n---------------------'\n",
    "    complete_text = ''.join([sent for sent in sentences])\n",
    "    print str.replace(str.replace(complete_text,'<P>',''),'</P>','').strip()\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_summaries(prob):\n",
    "    '''Generate summaries for approx ( (1-prob) *total_docs) number of documents\n",
    "       total_docs = 300 approx.\n",
    "       prob should be in the range : 0 <= prob <= 1.0 . 0 indicates all documents, 1 indicates none of the documents'''\n",
    "    \n",
    "    i = 1\n",
    "    for clust,docs in clust_files.items():\n",
    "        for doc in docs:        \n",
    "            if np.random.uniform(low=0.0, high=1.0) > prob and doc not in docs_without_summ[clust] :\n",
    "                generate_summary(clust,doc,clf,2,i,sents_in_summ=2)\n",
    "                i += 1\n",
    "                \n",
    "    print 'Generation Complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GENERATED SUMMARY for doc  1 ::\n",
      "-----------------------------------\n",
      "President Reagan appointed an interagency group to deal with the drought and Agriculture Secretary Richard Lyng authorized farmers in drought parched counties in 13 states to harvest hay on idled crop land in the government's Conservation Reserve Program.White House spokesman Marlin Fitzwater said that the president is \"very concerned\" about the drought, and \"wants to make certain that everything that the federal government can do to assist will be done.\"\n",
      "\n",
      "\n",
      "ACTUAL SUMMARY for doc  1 ::\n",
      "-----------------------------------\n",
      "The Reagan administration has moved to deal with the worsening drought by appointing an interagency group and authorizing farmers in dry counties to harvest hay on Conservation Reserve Program land. Thirteen states are now involved. A White House spokesman said the drought would drive up food prices, but not have overall implications for the economy. There was speculation that the export enhancement program would be curbed and embargos placed on soybean exports. Bankers are concerned about payments on farm loans and fewer loan applications. Farm equipment makers fear sales will drop, while realtors fear an end to the land boom.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "GENERATED SUMMARY for doc  2 ::\n",
      "-----------------------------------\n",
      "\"Right now the rains have caused everything to green up, but that can\n",
      "change in a few weeks,\" Lisa Boyd, a spokeswoman for the California Department\n",
      "of Forestry and Fire Protection in Sacramento, said Monday.\";    Mauldin said the Morgan Hill ranger\n",
      "unit will begin manning some of its seasonal stations Monday, begin hiring\n",
      "summer season firefighters June 3, and have all of the back-country stations\n",
      "opened by July 1.;    The Morgan Hill unit covers parts of Santa Clara,\n",
      "Alameda, Contra Costa, Stanislaus, San Joaquin and Merced counties.\n",
      "\n",
      "\n",
      "ACTUAL SUMMARY for doc  2 ::\n",
      "-----------------------------------\n",
      "Because of a five-year drought and rains that nurtured grass now turning dry, California is gearing up for a dangerous wildfire season. Firefighters are most concerned about certain rural areas. Residents have been advised to take special precautions; notably, clearing a 30-foot defensible space of dry grass and brush around the house; removing all pine needles and leaves from roofs, eaves and gutters; trimming tree limbs within 10 feet of the chimney and all limbs hanging over the house or garage; treating wood shingle roofs with fire retardant; making sure of adequate access for firefighters; ensuring that neighborhood streets are clearly identified.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "GENERATED SUMMARY for doc  3 ::\n",
      "-----------------------------------\n",
      "They surrounded the airport near Ljubljana, Slovenia's\n",
      "capital, and retook Slovenia's main border crossings with\n",
      "Austria, Italy and Hungary, effectively cutting off\n",
      "international traffic.But Slovene\n",
      "Defense Minister Janez Jansa said in a television interview\n",
      "that fighting was going on in at least 20 places in Slovenia,\n",
      "and he estimated there were more than 100 dead and wounded on\n",
      "both sides.\n",
      "\n",
      "\n",
      "ACTUAL SUMMARY for doc  3 ::\n",
      "-----------------------------------\n",
      "Two days after Slovenia and Croatia seceded from Yugoslavia, the federal government mobilized troops to regain control, beginning with Austrian border crossings. The Slovenian militia, formed in the past several months, is outmatched by the army's 180,000 troops, armored vehicles and air force. Fighting across Slovenia resulted in 100 dead and wounded. The West has lobbied for maintenance of the Yugoslav state, threatening to withhold aid if the country breaks up. Serbia fears losing the economic support of Slovenia and Croatia, who supply most of Yugoslavia's wealth and have tired of the erosion of their finances by the Serbian-dominated bureaucracy.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "GENERATED SUMMARY for doc  4 ::\n",
      "-----------------------------------\n",
      "died in a light plane crash in \n",
      "Mississippi, authorities said Monday, making him the second member of the House \n",
      "killed in an aviation accident in a week.\n",
      "\n",
      "At the White House, spokesman Marlin Fitzwater said President and Mrs. Bush \n",
      "\"deeply regret\" Smith's death, adding that Smith, who was elected to a seat \n",
      "vacated by Sen. Trent Lott (R-Miss.\n",
      "\n",
      "\n",
      "ACTUAL SUMMARY for doc  4 ::\n",
      "-----------------------------------\n",
      "A single-engine Cessna 177 has crashed in Janice, Mississippi, carrying Representative Larkin Smith of that state and his pilot to their deaths. They were flying from Hattiesburg to Gulfport after Smith had thrown out the first ball at the Dixie Youth World Series baseball tournament. The plane cut a swath 300 feet long through the woods where the wreckage was found. Searchers had spent all of Sunday night combing the woods, but the plane was finally spotted from the air. No cause has been determined. Smith was the second congressman in a week to die in a plane crash. Wreckage of Representative Mickey Leland's plane was located on the same day Representative Smith was killed.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "GENERATED SUMMARY for doc  5 ::\n",
      "-----------------------------------\n",
      "A single-engine airplane crashed\n",
      "Tuesday into a ditch beside a dirt road on the outskirts of\n",
      "Albuquerque, killing all five people aboard, authorities said.Four adults and one child died in the crash, which witnesses\n",
      "said occurred about 5 p.m., when it was raining, Albuquerque police\n",
      "Sgt.\n",
      "\n",
      "\n",
      "ACTUAL SUMMARY for doc  5 ::\n",
      "-----------------------------------\n",
      "A single engine Cessna P210 crashed into a ditch on the outskirts of Albuquerque, New Mexico in squally weather, killing four adults and a child. The pilot had previously attempted a landing, but aborted the try. He was circling the field to try again when the crash occurred. There was no fire, but the plane cartwheeled and all aboard were killed. An eye-witness, Walter Ramazinni, said that the plane had banked left when a gust of wind caught him. He banked to the right, then left, and crashed. The National Transportation Safety Board will investigate.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "GENERATED SUMMARY for doc  6 ::\n",
      "-----------------------------------\n",
      "Police captured the top military leader of the\n",
      "Shining Path, a Maoist rebel group whose eight-year guerrilla war\n",
      "has taken more than 10,000 lives in Peru, officials said Monday.Officials said the capture of Osman Morote, 43, considered the\n",
      "most radical leader of the movement, was the hardest blow to date\n",
      "for the rebels since they launched their guerrilla war in the\n",
      "Andean highlands in May 1980.\n",
      "\n",
      "\n",
      "ACTUAL SUMMARY for doc  6 ::\n",
      "-----------------------------------\n",
      "On Sunday, the Peruvian Police captured Osman Morote, the top military leader of the Shining Path, a Maoist rebel group. Morote had arrived in Lima a week ago to coordinate terrorist attacks in Lima. The Shining Path was founded by Abimael Guzman. It is a splinter of the Peruvian Communist Party and is seeking to overthrow Peru's elected government and install a peasant and worker state. Eight years ago the group took up arms after conducting semi-clandestine political work with peasants in the Andean highlands for 10 years. Guzman, not seen for years, still is recognized at the group's leader.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "GENERATED SUMMARY for doc  7 ::\n",
      "-----------------------------------\n",
      "According to Associated Press reports, some witnesses to the crash, including an air traffic controller, said they saw blue flame coming from one of the jetliner's engines just before it hit the ground and exploded into a fireball.For example, Sen. Byrd and Sen. Ernest Hollings (D., S.C.), chairman of the Senate Commerce Committee, have already drawn up legislation that would require small planes to be equipped with altitude-reporting devices at hundreds of airports across the U.S. \n",
      "\n",
      "   Sen. Frank Lautenberg (D., N.J.), chairman of the Senate Appropriations Subcommittee on Transportation, predicted that the accident will \"accelerate movement by the Congress and the Transportation Department to do whatever can be done -- either by hiring more maintenance inspectors or reducing air traffic to meet the capacity of the system, to reduce the chances of future accidents.\"\n",
      "\n",
      "\n",
      "ACTUAL SUMMARY for doc  7 ::\n",
      "-----------------------------------\n",
      "The MD82 Northwest Airline plane that crashed on takeoff August 16 in Detroit had two previous incidents of engine failure, and was forced to land in 1985 and 1986 following power loss in an engine. Cause of this crash has not been ascertained but engine failure seems likely. 154 were killed; a surviving child is in critical condition. The plane had a heavy fuel load and had ten more passengers on board than its seat limit. Engines of the type on the MD82 have had problems in the past and the FBI is investigating reports of engine tampering.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "GENERATED SUMMARY for doc  8 ::\n",
      "-----------------------------------\n",
      "``Mad cow disease'' has killed 10,000 cattle,\n",
      "restricted the export market for Britain's cattle industry and\n",
      "raised fears about the safety of eating beef.``I think it is a recognition _ not just of pressure from\n",
      "farmers _ but that the public would feel more confident that no\n",
      "BSE-infected animal would ever be likely to go anywhere near the\n",
      "food chain if there was 100 percent compensation,'' said Sir Simon\n",
      "Gourlay, president of the National Farmers Union.\n",
      "\n",
      "\n",
      "ACTUAL SUMMARY for doc  8 ::\n",
      "-----------------------------------\n",
      "\"Mad cow disease\" (bovine spongiform encephalopathy or BSE) has killed 10,000 cattle, restricted the export market for British beef and raised doubts about the safety of eating beef. Although the British government insists that the incurable disease poses only a remote risk to human health, scientists are not sure what causes the disease or how it is transmitted. It has symptoms similar to scrapie, a disease long observed in sheep, eating holes in the brains of its victims and in its late stages causing an animal to stagger drunkenly. There are two human forms of spongiform encephalopathy: Creutzfeldt- Jakob disease and kuru.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "GENERATED SUMMARY for doc  9 ::\n",
      "-----------------------------------\n",
      "Richard Lacey, a microbiology professor at Leeds University and the leading\n",
      "critic of government policy on BSE, said the deaths this year of two farmers\n",
      "whose herds had suffered from mad cow disease could not be put down to\n",
      "chance.The advisers, led by the government's chief medical officer Kenneth Calman\n",
      "and chief veterinary officer Keith Meldrum, put out a detailed statement to\n",
      "justify their view that last month's death of 65-year-old Duncan Templeman -\n",
      "following that of Peter Warhurst, 61, a year ago - showed 'no features that\n",
      "give cause for undue concern'.\n",
      "\n",
      "\n",
      "ACTUAL SUMMARY for doc  9 ::\n",
      "-----------------------------------\n",
      "Bovine spongiform encephalopathy (BSE) which has killed more than 100,000 animals in the UK is causing increased public concern. A second dairy farmer has died of a related human brain disorder, Creutzfeld-Jacob disease (CJD) causing some to maintain that BSE can trigger CJD. BSE, CJD and scrapie, the ovine form of the disease, are all caused by an abnormal form of protein, the prion, that converts normal protein into its own abnormal form. BSE can be transmitted by injecting or eating large amounts of infected tissue, but no human is likely to be exposed to BSE in sufficient quantities to develop the disease.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "GENERATED SUMMARY for doc  10 ::\n",
      "-----------------------------------\n",
      "\"Mad cow disease\" has killed 10,000 cattle, restricted the export market for \n",
      "Britain's cattle industry and raised fears about the safety of eating beef.\n",
      "\n",
      "\"I think it is a recognition -- not just of pressure from farmers -- but that \n",
      "the public would feel more confident that no BSE-infected animal would ever be \n",
      "likely to go anywhere near the food chain if there was 100% compensation,\" said \n",
      "Sir Simon Gourlay, president of the National Farmers Union.\n",
      "\n",
      "\n",
      "ACTUAL SUMMARY for doc  10 ::\n",
      "-----------------------------------\n",
      "Scientists are uncertain of the cause or means of transmission of \"Mad cow disease\" which has killed 10,000 cattle, adversely affecting the market for British beef and raising fears about eating beef. The British government has taken steps to stop the spread of the disease, while compensating owners for animals destroyed, and assuring the public that there is little chance of risk to human health. Nevertheless, several countries have imposed an embargo on the export of cattle from Britain. Two dozen cases of a human disease related to Mad Cow disease were reported in Britain last year.\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Generation Complete\n"
     ]
    }
   ],
   "source": [
    "random_summaries(0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int32 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale(np.array([[1,2,3],[4,5,6]])).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
